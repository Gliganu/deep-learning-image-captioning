{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge,BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras import callbacks\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import preprocessing as preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_images_by(searched_word,images,predicted_captions):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemm_word = lmtzr.lemmatize(searched_word)\n",
    "    \n",
    "    found_indexes = []\n",
    "    for index,caption in enumerate(predicted_captions):\n",
    "        lemm_caption_words = [lmtzr.lemmatize(word) for word in caption.split()]\n",
    "        if lemm_word in lemm_caption_words:\n",
    "            found_indexes.append(index)\n",
    "    \n",
    "    return ([images[i] for i in found_indexes],[predicted_captions[i] for i in found_indexes])\n",
    "\n",
    "    \n",
    "def make_prediction(random_number,images_concat_t,vgg_model):\n",
    "    startIndex = word2index[\"START\"]\n",
    "    start_captions = [[startIndex]]\n",
    "    start_captions = sequence.pad_sequences(start_captions, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "\n",
    "    firstImage = np.expand_dims(images_concat_t[random_number], axis=0)\n",
    "\n",
    "    first_image_vgg_features = vgg_model.predict(firstImage)\n",
    "#     first_image_input = firstImage\n",
    "    first_image_input = np.squeeze(first_image_vgg_features)[0].reshape(1,4096)\n",
    "    \n",
    "    firstCaption = np.expand_dims(start_captions[0], axis=0) \n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    endGenerated = False\n",
    "    i = 0\n",
    "    while ((not endGenerated) & (i < MAX_CAPTION_LEN-1)):\n",
    "\n",
    "        predictions = model.predict([first_image_input, firstCaption])\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        currentPred = predictions[i]\n",
    "\n",
    "        max_index = np.argmax(currentPred)\n",
    "\n",
    "        outputs.append(max_index)\n",
    "        firstCaption[0,i+1] = max_index\n",
    "\n",
    "        i+=1\n",
    "\n",
    "        if(index2word[max_index] == \"END\"):\n",
    "            endGenerated = True\n",
    "\n",
    "    caption = ' '.join([index2word[x] for x in firstCaption[0][:i+1]]) \n",
    "    \n",
    "    drawImage = firstImage[0]\n",
    "    drawImageT = np.transpose(drawImage,(1,2,0))\n",
    "    plt.imshow(drawImageT)\n",
    "    \n",
    "    return (drawImageT,caption)\n",
    "\n",
    "def make_prediction_on_dataset(images_concat_t, window_start = None, no_images = None):\n",
    "    \n",
    "    if(window_start == None):\n",
    "        window_start = 0\n",
    "        \n",
    "    if(no_images == None):\n",
    "        no_images = len(images_concat_t)\n",
    "    \n",
    "    vgg_model = get_vgg_model()\n",
    "    \n",
    "    images2Captions = [make_prediction(i,images_concat_t,vgg_model) for i in tqdm(range(window_start,window_start+no_images))]\n",
    "    images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "    predicted_captions = [image2Caption[1] for image2Caption in images2Captions]\n",
    "\n",
    "    \n",
    "    return (images,predicted_captions)\n",
    "    \n",
    "def generate_arrays_from_file(img_vgg_path,indexed_captions_path,future_words_path):\n",
    "    while 1:\n",
    "        img_vgg_elements = os.listdir(img_vgg_path)\n",
    "        indexed_captions_elements = os.listdir(indexed_captions_path)\n",
    "        future_words_elements = os.listdir(future_words_path)\n",
    "        \n",
    "        img_vgg_elements.sort()\n",
    "        indexed_captions_elements.sort()\n",
    "        future_words_elements.sort()\n",
    "\n",
    "        nr_elem = len(img_vgg_elements)\n",
    "        \n",
    "        BATCH_SIZE = 1\n",
    "        \n",
    "        for index in range(nr_elem/BATCH_SIZE):\n",
    "            \n",
    "            img_vgg_batch_list = []\n",
    "            indexed_caption_batch_list = []\n",
    "            future_words_batch_list = []\n",
    "            \n",
    "            for elem_in_batch in range(BATCH_SIZE):\n",
    "                \n",
    "                img_vgg_el_name = img_vgg_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "                indexed_caption_name = indexed_captions_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "                future_words_el_name = future_words_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "\n",
    "                img_vgg = preproc.load_array(img_vgg_path+\"/\"+img_vgg_el_name)\n",
    "                indexed_caption = preproc.load_array(indexed_captions_path+\"/\"+indexed_caption_name)\n",
    "                future_words = preproc.load_array(future_words_path+\"/\"+future_words_el_name)\n",
    "                \n",
    "                img_vgg_batch_list.append(img_vgg)\n",
    "                indexed_caption_batch_list.append(indexed_caption)\n",
    "                future_words_batch_list.append(future_words)\n",
    "                \n",
    "            img_vgg_big = np.vstack(img_vgg_batch_list)\n",
    "            indexed_caption_big = np.vstack(indexed_caption_batch_list)\n",
    "            future_words_big = np.vstack(future_words_batch_list)\n",
    "            \n",
    "#             print(img_vgg_big.shape)\n",
    "#             print(indexed_caption_big.shape)\n",
    "#             print(future_words_big.shape)\n",
    "    \n",
    "            yield ([img_vgg_big,indexed_caption_big], future_words_big)\n",
    "\n",
    "def get_test_data(img_vgg_path,indexed_captions_path,future_words_path):\n",
    "    img_vgg_elements = os.listdir(img_vgg_path)\n",
    "    indexed_captions_elements = os.listdir(indexed_captions_path)\n",
    "    future_words_elements = os.listdir(future_words_path)\n",
    "\n",
    "    img_vgg_elements.sort()\n",
    "    indexed_captions_elements.sort()\n",
    "    future_words_elements.sort()\n",
    "\n",
    "    nr_batches = 1\n",
    "\n",
    "    for index in tqdm(range(nr_batches)):\n",
    "\n",
    "        img_vgg_batch_list = []\n",
    "        indexed_caption_batch_list = []\n",
    "        future_words_batch_list = []\n",
    "\n",
    "        img_vgg_el_name = img_vgg_elements[index]\n",
    "        indexed_caption_name = indexed_captions_elements[index]\n",
    "        future_words_el_name = future_words_elements[index]\n",
    "\n",
    "        img_vgg = preproc.load_array(img_vgg_path+\"/\"+img_vgg_el_name)\n",
    "        indexed_caption = preproc.load_array(indexed_captions_path+\"/\"+indexed_caption_name)\n",
    "        future_words = preproc.load_array(future_words_path+\"/\"+future_words_el_name)\n",
    "\n",
    "        img_vgg_batch_list.append(img_vgg)\n",
    "        indexed_caption_batch_list.append(indexed_caption)\n",
    "        future_words_batch_list.append(future_words)\n",
    "\n",
    "    img_vgg_big = np.vstack(img_vgg_batch_list)\n",
    "    indexed_caption_big = np.vstack(indexed_caption_batch_list)\n",
    "    future_words_big = np.vstack(future_words_batch_list)\n",
    "\n",
    "    print(img_vgg_big.shape)\n",
    "    print(indexed_caption_big.shape)\n",
    "    print(future_words_big.shape)\n",
    "\n",
    "    return img_vgg_big, indexed_caption_big, future_words_big\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = app_3_length_15_data_path\n",
    "\n",
    "train_path = base_path + train_folder\n",
    "val_path = base_path + val_folder\n",
    "\n",
    "NR_TRAIN_EXAMPLES = 62735\n",
    "NR_TEST_EXAMPLES = 28858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28858, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "test_images_concat_t = preproc.get_images_concat(val_path + images_concat_folder+ 'images_concat.bc',NR_TEST_EXAMPLES)\n",
    "print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load precomputed misc data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img_vgg_path = base_path + train_folder + batch_folder + images_vgg_4096_folder\n",
    "train_indexed_captions_path = base_path + train_folder + batch_folder + indexed_captions_folder\n",
    "train_raw_captions_path = base_path + train_folder + batch_folder + captions_folder\n",
    "train_future_words_path = base_path + train_folder + batch_folder + indexed_future_words_folder\n",
    "\n",
    "test_img_vgg_path = base_path + val_folder + batch_folder +images_vgg_4096_folder\n",
    "test_indexed_captions_path = base_path + val_folder + batch_folder + indexed_captions_folder\n",
    "test_raw_captions_path = base_path + val_folder + batch_folder+captions_folder\n",
    "test_future_words_path = base_path + val_folder + batch_folder+indexed_future_words_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 170.56it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 644.24it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_words = preproc.load_obj(base_path + general_datastruct_folder+\"unique_words\")\n",
    "word2index = preproc.load_obj(base_path+general_datastruct_folder+\"word2index\")\n",
    "index2word = preproc.load_obj(base_path+general_datastruct_folder+\"index2word\")\n",
    "\n",
    "(train_captions_raw,_) = preproc.get_captions_raw_and_indexed(train_raw_captions_path,train_indexed_captions_path)\n",
    "(test_captions_raw,_) = preproc.get_captions_raw_and_indexed(test_raw_captions_path,test_indexed_captions_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_words)\n",
    "MAX_CAPTION_LEN = 15 # ATENTIE AICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_CAPTION_LEN = 15\n",
      "VOCAB_SIZE = 7275\n"
     ]
    }
   ],
   "source": [
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found = 7025\n",
      "Not found = 249\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 200\n",
    "vecs, words, wordidx = preproc.load_vectors(save_path+glove_folder+\"6B.\"+str(EMB_SIZE)+\"d\")\n",
    "\n",
    "emb = preproc.create_emb(vecs, words, wordidx,index2word,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VGG\n",
    "def get_vgg_model():\n",
    "    image_model = Vgg16().model\n",
    "    image_model.pop()\n",
    "    image_model.pop()\n",
    "    image_model.trainable = False\n",
    "    image_model.add(RepeatVector(MAX_CAPTION_LEN))\n",
    "    return image_model\n",
    "\n",
    "def get_precomputed_input_model():\n",
    "    input_model = Sequential()\n",
    "    input_model.add(RepeatVector(MAX_CAPTION_LEN,input_shape=(4096,)))\n",
    "    return input_model\n",
    "\n",
    "\n",
    "# GRU\n",
    "def get_language_model(emb):\n",
    "    language_model = Sequential()\n",
    "    #language_model.add(Embedding(VOCAB_SIZE, 256, input_length=MAX_CAPTION_LEN))\n",
    "    #language_model.add(Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_CAPTION_LEN,weights=[emb], trainable=False))\n",
    "    language_model.add(Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_CAPTION_LEN,weights=[emb]))\n",
    "    Dropout(0.2)\n",
    "    language_model.add(BatchNormalization())\n",
    "    return language_model\n",
    "\n",
    "\n",
    "# Top level model\n",
    "def build_model(image_model,language_model):\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, language_model], mode='concat'))\n",
    "    model.add(GRU(1024, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE, activation = 'softmax')))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_model = get_precomputed_input_model()\n",
    "language_model = get_language_model(emb)\n",
    "model = build_model(image_model,language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_img_vgg, test_indexed_captions, test_future_words = get_test_data(test_img_vgg_path,\n",
    "                                                                       test_indexed_captions_path,\n",
    "                                                                       test_future_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "                    generate_arrays_from_file(train_img_vgg_path,train_indexed_captions_path,train_future_words_path),\n",
    "                    samples_per_epoch=2000,\n",
    "                    nb_epoch=3,\n",
    "                    validation_data = ([test_img_vgg, test_indexed_captions], test_future_words),\n",
    "                    callbacks=[callbacks.RemoteMonitor()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.plot_loss_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.save_weights(save_path + models_folder +'app_3_length_15_30_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(save_path + models_folder +'app_3_length_15_30_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "repeatvector_1 (RepeatVector)    (None, 15, 4096)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 15, 200)       1455000                                      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 15, 200)       800                                          \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 15, 1024)      16346112    merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 15, 7275)      7456875     gru_1[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 25,258,787\n",
      "Trainable params: 25,258,387\n",
      "Non-trainable params: 400\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_arrays_from_file(train_img_vgg_path,\n",
    "                                                   train_indexed_captions_path,\n",
    "                                                   train_future_words_path),\n",
    "                        val_samples = NR_TRAIN_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate_generator(generate_arrays_from_file(test_img_vgg_path,\n",
    "                                                   test_indexed_captions_path,\n",
    "                                                   test_future_words_path),\n",
    "                        val_samples = NR_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_predictions(test_images_concat_t):\n",
    "    return make_prediction_on_dataset(test_images_concat_t,0,NR_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(_,all_test_predictions) = make_test_predictions(test_images_concat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.save_obj(all_test_predictions,val_path+\"predictions/\"+\"app_3_length_15_30_epoch_predicted_captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n"
     ]
    }
   ],
   "source": [
    "window_start = 512\n",
    "nr_images = 128\n",
    "\n",
    "# images_concat_t = train_images_concat_t\n",
    "# real_captions = train_captions_raw\n",
    "\n",
    "images_concat_t = test_images_concat_t\n",
    "real_captions = test_captions_raw\n",
    "\n",
    "(images,predicted_captions) = make_prediction_on_dataset(images_concat_t,window_start,nr_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.plot_predictions(images,titles = predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words2app = preproc.most_common_words(predicted_captions,500)\n",
    "common_words2app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_words2app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searched_word = \"teeth\"\n",
    "(found_images,found_captions) = search_images_by(searched_word,images,predicted_captions)\n",
    "print(\"Number of results = %d\"%len(found_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(found_images,found_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on misc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images_path = save_path + misc_images_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images = []\n",
    "for img_path in os.listdir(misc_images_path):\n",
    "    img = PIL.Image.open(misc_images_path+img_path)\n",
    "    img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "    img = np.asarray(img)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    misc_images.append(img)\n",
    "    \n",
    "stacked_images = np.vstack(misc_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (misc_images,misc_predicted_captions) = make_prediction_on_dataset(stacked_images)\n",
    "# preproc.plot_predictions(misc_images,misc_predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
