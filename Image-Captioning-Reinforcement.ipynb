{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge,BatchNormalization,Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Embedding,LSTM,GRU,TimeDistributed,RepeatVector,Merge,Input,merge,UpSampling2D\n",
    "from keras.preprocessing import sequence\n",
    "from keras import callbacks\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from numpy.random import random, permutation, randn, normal \n",
    "\n",
    "import os\n",
    "\n",
    "import preprocessing as preproc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import languageUtils\n",
    "import nnModel\n",
    "import nnModel_no_feedback\n",
    "import plotter\n",
    "import videoExplorer as vidExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = data_path+\"app-100-length-15/\"\n",
    "\n",
    "train_path = base_path + train_folder\n",
    "val_path = base_path + val_folder\n",
    "\n",
    "NR_TRAIN_EXAMPLES = 219136\n",
    "NR_TEST_EXAMPLES = 28858\n",
    "SMALL_NR_TEST_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "test_images_concat_t = preproc.get_images_concat(\n",
    "    val_path + images_concat_folder+ 'images_concat_0.bc',\n",
    "    SMALL_NR_TEST_EXAMPLES)\n",
    "\n",
    "print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load precomputed misc data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img_vgg_path = base_path + train_folder + batch_folder + images_vgg_4096_folder\n",
    "train_indexed_captions_path = base_path + train_folder + batch_folder + indexed_captions_folder\n",
    "train_raw_captions_path = base_path + train_folder + batch_folder + captions_folder\n",
    "train_future_words_path = base_path + train_folder + batch_folder + indexed_future_words_folder\n",
    "\n",
    "test_img_vgg_path = base_path + val_folder + batch_folder +images_vgg_4096_folder\n",
    "test_indexed_captions_path = base_path + val_folder + batch_folder + indexed_captions_folder\n",
    "test_raw_captions_path = base_path + val_folder + batch_folder+captions_folder\n",
    "test_future_words_path = base_path + val_folder + batch_folder+indexed_future_words_folder\n",
    "\n",
    "\n",
    "train_prev_captions_path = base_path + train_folder + batch_folder + indexed_prev_captions_folder\n",
    "test_prev_captions_path = base_path + val_folder + batch_folder + indexed_prev_captions_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 665.98it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 660.97it/s]\n"
     ]
    }
   ],
   "source": [
    "(unique_words, word2index, index2word) = languageUtils.load_language_data_structures(base_path + general_datastruct_folder)\n",
    "\n",
    "(train_captions_raw,_) = preproc.get_captions_raw_and_indexed(train_raw_captions_path,train_indexed_captions_path)\n",
    "(test_captions_raw,_) = preproc.get_captions_raw_and_indexed(test_raw_captions_path,test_indexed_captions_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_CAPTION_LEN = 15\n",
      "VOCAB_SIZE = 849\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 300\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "MAX_CAPTION_LEN = 15 # ATENTIE AICI\n",
    "\n",
    "\n",
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb = nnModel.get_embeddings(index2word, VOCAB_SIZE, EMB_SIZE)\n",
    "model = nnModel.build_model(emb,MAX_CAPTION_LEN, VOCAB_SIZE, EMB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "repeatvector_1 (RepeatVector)    (None, 15, 4096)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 15, 300)       254700                                       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 15, 300)       1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 15, 300)       254700                                       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 15, 300)       1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 15, 1024)      17574912    merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 15, 1024)      0           gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 15, 1024)      6294528     dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 15, 849)       870225      gru_2[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 25,251,465\n",
      "Trainable params: 25,250,265\n",
      "Non-trainable params: 1,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 18432/219136 [=>............................] - ETA: 170s - loss: 8.8022 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-709b4803bc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     callbacks=[#callbacks.RemoteMonitor(),\n\u001b[0;32m---> 10\u001b[0;31m                                callbacks.CSVLogger(\"./training.log\")]\n\u001b[0m\u001b[1;32m     11\u001b[0m                    )\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "                    nnModel.generate_arrays_from_file(train_img_vgg_path,train_indexed_captions_path,\n",
    "                                              train_future_words_path,train_prev_captions_path),\n",
    "                    samples_per_epoch = 219136,\n",
    "                    nb_epoch=10,\n",
    "                    validation_data = nnModel.generate_arrays_from_file(test_img_vgg_path,test_indexed_captions_path,\n",
    "                                                   test_future_words_path,test_prev_captions_path),\n",
    "                    nb_val_samples = 2048,\n",
    "                    callbacks=[#callbacks.RemoteMonitor(),\n",
    "                               callbacks.CSVLogger(\"./training.log\")]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(nnModel.generate_arrays_from_file(test_img_vgg_path,test_indexed_captions_path,\n",
    "                                                   test_future_words_path,test_prev_captions_path),\n",
    "                         val_samples = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.save_weights(save_path + models_folder+\"big/\" +'app_100_length_15_past_word_30_epoch_300d_gru_2x1048_big.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(save_path + models_folder+\"big/\" +'app_100_length_15_past_word_20_epoch_300d_gru_2x1048_big.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(nnModel.generate_arrays_from_file(train_img_vgg_path,\n",
    "                                                   train_indexed_captions_path,\n",
    "                                                   train_future_words_path,\n",
    "                                                   train_prev_captions_path),\n",
    "                        val_samples = NR_TRAIN_EXAMPLES,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_arrays_from_file(test_img_vgg_path,\n",
    "                                                   test_indexed_captions_path,\n",
    "                                                   test_future_words_path,\n",
    "                                                   test_prev_captions_path),\n",
    "                        val_samples = NR_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_start = 200\n",
    "nr_images = 10\n",
    "\n",
    "# images_concat_t = train_images_concat_t #ATENTIE\n",
    "# real_captions = train_captions_raw\n",
    "\n",
    "images_concat_t = test_images_concat_t\n",
    "real_captions = languageUtils.strip_caption_list(test_captions_raw)\n",
    "\n",
    "print(images_concat_t.shape)\n",
    "\n",
    "# (images,predicted_captions) = nnModel.make_prediction_on_dataset(images_concat_t,window_start,nr_images)\n",
    "(images,predicted_captions) = nnModel.make_prediction_on_dataset(images_concat_t,model,word2index,index2word,MAX_CAPTION_LEN,window_start,nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_captions = [str(a) +\" ------ \" + str(b)  for a,b in zip(predicted_captions,real_captions)]\n",
    "# merged_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotter.plot_predictions(images,titles = predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words2app = languageUtils.most_common_words(predicted_captions,500)\n",
    "common_words2app[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searched_word = \"wave\"\n",
    "(found_images,found_captions,_) = vidExplorer.search_video_by(searched_word,images,predicted_captions)\n",
    "print(\"Number of results = %d\"%len(found_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter.plot_predictions(found_images,found_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Meetup Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_start = 200\n",
    "nr_images = 10\n",
    "images_concat_t = test_images_concat_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction(random_number,images_concat_t,vgg_model,model,word2index, index2word, MAX_CAPTION_LEN):\n",
    "    \n",
    "    startIndex = word2index[\"START\"]\n",
    "    start_captions = [[startIndex]]\n",
    "    start_captions = sequence.pad_sequences(start_captions, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "\n",
    "    img = np.expand_dims(images_concat_t[random_number], axis=0)\n",
    "    img_vgg_features = vgg_model.predict(img)\n",
    "    img_vgg_features = np.squeeze(img_vgg_features)[0].reshape(1,4096)\n",
    "    \n",
    "    indexed_caption = np.expand_dims(start_captions[0], axis=0) \n",
    "    prev_word_indexed_captions = np.expand_dims(list(start_captions[0]), axis=0)\n",
    "\n",
    "    reached_end = False\n",
    "    i = 0\n",
    "        \n",
    "    while ((not reached_end) & (i < MAX_CAPTION_LEN-1)):\n",
    "       \n",
    "        predictions = model.predict([img_vgg_features, indexed_caption, prev_word_indexed_captions])\n",
    "        predictions = predictions[0]\n",
    "        \n",
    "        currentPred = predictions[i]\n",
    "        \n",
    "        max_index = np.argmax(currentPred)\n",
    "        \n",
    "        indexed_caption[0,i+1] = max_index\n",
    "        \n",
    "        prev_word_indexed_captions[0,i+1] = indexed_caption[0,i]\n",
    "                \n",
    "        i+=1\n",
    "\n",
    "        if(index2word[max_index] == \"END\"):\n",
    "            reached_end = True\n",
    "\n",
    "    caption = ' '.join([index2word[x] for x in indexed_caption[0][1:i]])\n",
    "    \n",
    "    return (img[0],caption)\n",
    "\n",
    "def make_prediction_on_dataset(images_concat_t, model, word2index, index2word, MAX_CAPTION_LEN,  \n",
    "                               window_start = None, no_images = None):\n",
    "    \n",
    "    if(window_start == None):\n",
    "        window_start = 0\n",
    "        \n",
    "    if(no_images == None):\n",
    "        no_images = len(images_concat_t)\n",
    "\n",
    "    vgg_model = nnModel.get_vgg_model(MAX_CAPTION_LEN)\n",
    "    \n",
    "    images2Captions = [make_prediction(i,images_concat_t,vgg_model,model,word2index, index2word, MAX_CAPTION_LEN) \n",
    "                       for i in tqdm(range(window_start,window_start+no_images))]\n",
    "    \n",
    "    images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "    predicted_captions = [image2Caption[1] for image2Caption in images2Captions]\n",
    "\n",
    "    images = [np.transpose(img,(1,2,0)) for img in images]\n",
    "        \n",
    "    return (images,predicted_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(images,predicted_captions) = make_prediction_on_dataset(images_concat_t,model,word2index,index2word,MAX_CAPTION_LEN,window_start,nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on misc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images_path = save_path + misc_images_folder\n",
    "# misc_images_path = save_path + \"telenav-images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images = []\n",
    "for img_path in os.listdir(misc_images_path):\n",
    "    img = PIL.Image.open(misc_images_path+img_path)\n",
    "    img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "    img = np.asarray(img)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    misc_images.append(img)\n",
    "    \n",
    "stacked_images = np.vstack(misc_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(misc_images,misc_predicted_captions) = nnModel.make_prediction_on_dataset(stacked_images,model,word2index,index2word,MAX_CAPTION_LEN,0,len(stacked_images))\n",
    "plotter.plot_predictions(misc_images,misc_predicted_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare data for \"Caption Turing Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_captions(image_data_arr):\n",
    "    caption_bucket_list = preproc.get_captions_list(image_data_arr)\n",
    "    caption_bucket_length = len(caption_bucket_list[0])\n",
    "    \n",
    "    captions = np.stack(caption_bucket_list)\n",
    "    \n",
    "    print(\"caption_bucket_length = %d\"%caption_bucket_length)\n",
    "    print(\"captions.shape = %s\"%str(captions.shape))\n",
    "    \n",
    "    return (caption_bucket_list)\n",
    "\n",
    "\n",
    "def get_imgs_and_captions(base_images_path,base_annotation_path,nr_items):\n",
    "    image_data_arr = preproc.get_image_data_arr(base_images_path,base_annotation_path,nr_items)\n",
    "    \n",
    "    captions= get_all_captions(image_data_arr)\n",
    "    images = preproc.construct_images_concat_t(image_data_arr)\n",
    "    \n",
    "    print(\"images.shape = %s\"%str(images.shape))\n",
    "    return (images,captions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_images_path = val_images_path\n",
    "base_annotation_path = val_annotation_path\n",
    "\n",
    "(images,caption_bucket_list) = get_imgs_and_captions(base_images_path,base_annotation_path,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(_,predicted_captions) = make_prediction_on_dataset(images,0,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_captions_1 = strip_caption_list(caption_bucket_list[0])\n",
    "real_captions_2 = strip_caption_list(caption_bucket_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_df = percentile_list = pd.DataFrame(\n",
    "    {'image': list(images),\n",
    "     'predicted_caption': predicted_captions,\n",
    "     'real_caption_1': real_captions_1,\n",
    "     'real_caption_2': real_captions_2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
