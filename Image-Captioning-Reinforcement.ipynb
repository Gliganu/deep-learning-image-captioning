{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge,BatchNormalization,Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Embedding,LSTM,GRU,TimeDistributed,RepeatVector,Merge,Input,merge,UpSampling2D\n",
    "from keras.preprocessing import sequence\n",
    "from keras import callbacks\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils as utils\n",
    "\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from numpy.random import random, permutation, randn, normal \n",
    "\n",
    "import os\n",
    "\n",
    "import preprocessing as preproc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_images_by(searched_word,images,predicted_captions):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemm_word = lmtzr.lemmatize(searched_word)\n",
    "    \n",
    "    found_indexes = []\n",
    "    for index,caption in enumerate(predicted_captions):\n",
    "        lemm_caption_words = [lmtzr.lemmatize(word) for word in caption.split()]\n",
    "        if lemm_word in lemm_caption_words:\n",
    "            found_indexes.append(index)\n",
    "    \n",
    "    return ([images[i] for i in found_indexes],[predicted_captions[i] for i in found_indexes])\n",
    "\n",
    "    \n",
    "# def make_prediction(random_number,images_concat_t,vgg_model):\n",
    "#     startIndex = word2index[\"START\"]\n",
    "#     start_captions = [[startIndex]]\n",
    "#     start_captions = sequence.pad_sequences(start_captions, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "\n",
    "#     firstImage = np.expand_dims(images_concat_t[random_number], axis=0)\n",
    "#     first_image_vgg_features = vgg_model.predict(firstImage)\n",
    "#     first_image_input = np.squeeze(first_image_vgg_features)[0].reshape(1,4096)\n",
    "    \n",
    "#     firstCaption = np.expand_dims(start_captions[0], axis=0) \n",
    "#     prev_word_indexed_captions = np.expand_dims(list(start_captions[0]), axis=0)\n",
    "    \n",
    "#     outputs = []\n",
    "\n",
    "#     endGenerated = False\n",
    "#     i = 0\n",
    "        \n",
    "#     while ((not endGenerated) & (i < MAX_CAPTION_LEN-1)):\n",
    "       \n",
    "#         predictions = model.predict([first_image_input, firstCaption, prev_word_indexed_captions])\n",
    "#         predictions = predictions[0]\n",
    "        \n",
    "#         currentPred = predictions[i]\n",
    "        \n",
    "#         max_index = np.argmax(currentPred)\n",
    "        \n",
    "#         outputs.append(max_index)\n",
    "#         firstCaption[0,i+1] = max_index\n",
    "        \n",
    "#         prev_word_indexed_captions[0,i+1] = firstCaption[0,i]\n",
    "                \n",
    "#         i+=1\n",
    "\n",
    "#         if(index2word[max_index] == \"END\"):\n",
    "#             endGenerated = True\n",
    "\n",
    "#     caption = ' '.join([index2word[x] for x in firstCaption[0][1:i]])\n",
    "\n",
    "#     drawImage = firstImage[0]\n",
    "#     drawImageT = np.transpose(drawImage,(1,2,0))\n",
    "#     plt.imshow(drawImageT)\n",
    "    \n",
    "#     return (drawImageT,caption)\n",
    "\n",
    "# def make_prediction_on_dataset(images_concat_t, window_start = None, no_images = None):\n",
    "    \n",
    "#     if(window_start == None):\n",
    "#         window_start = 0\n",
    "        \n",
    "#     if(no_images == None):\n",
    "#         no_images = len(images_concat_t)\n",
    "    \n",
    "#     vgg_model = get_vgg_model()\n",
    "    \n",
    "#     images2Captions = [make_prediction(i,images_concat_t,vgg_model) for i in tqdm(range(window_start,window_start+no_images))]\n",
    "#     images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "#     predicted_captions = [image2Caption[1] for image2Caption in images2Captions]\n",
    "\n",
    "    \n",
    "#     return (images,predicted_captions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = data_path+\"app-100-length-15/\"\n",
    "\n",
    "train_path = base_path + train_folder\n",
    "val_path = base_path + val_folder\n",
    "\n",
    "NR_TRAIN_EXAMPLES = 219136\n",
    "NR_TEST_EXAMPLES = 28858\n",
    "SMALL_NR_TEST_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_images_concat_t = preproc.get_images_concat(\n",
    "#     val_path + images_concat_folder+ 'images_concat_0.bc',\n",
    "#     SMALL_NR_TEST_EXAMPLES)\n",
    "\n",
    "# print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load precomputed misc data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img_vgg_path = base_path + train_folder + batch_folder + images_vgg_4096_folder\n",
    "train_indexed_captions_path = base_path + train_folder + batch_folder + indexed_captions_folder\n",
    "train_raw_captions_path = base_path + train_folder + batch_folder + captions_folder\n",
    "train_future_words_path = base_path + train_folder + batch_folder + indexed_future_words_folder\n",
    "\n",
    "test_img_vgg_path = base_path + val_folder + batch_folder +images_vgg_4096_folder\n",
    "test_indexed_captions_path = base_path + val_folder + batch_folder + indexed_captions_folder\n",
    "test_raw_captions_path = base_path + val_folder + batch_folder+captions_folder\n",
    "test_future_words_path = base_path + val_folder + batch_folder+indexed_future_words_folder\n",
    "\n",
    "indexed_prev_captions_folder = \"indexed-prev-captions/\"\n",
    "train_prev_captions_path = base_path + train_folder + batch_folder + indexed_prev_captions_folder\n",
    "test_prev_captions_path = base_path + val_folder + batch_folder + indexed_prev_captions_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'utils.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 707.11it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 677.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# unique_words = preproc.load_obj(base_path + general_datastruct_folder+\"unique_words\")\n",
    "# word2index = preproc.load_obj(base_path+general_datastruct_folder+\"word2index\")\n",
    "# index2word = preproc.load_obj(base_path+general_datastruct_folder+\"index2word\")\n",
    "\n",
    "(unique_words, word2index, index2word) = utils.load_language_data_structures(base_path + general_datastruct_folder)\n",
    "\n",
    "(train_captions_raw,_) = preproc.get_captions_raw_and_indexed(train_raw_captions_path,train_indexed_captions_path)\n",
    "(test_captions_raw,_) = preproc.get_captions_raw_and_indexed(test_raw_captions_path,test_indexed_captions_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_words)\n",
    "MAX_CAPTION_LEN = 15 # ATENTIE AICI\n",
    "\n",
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found = 840\n",
      "Not found = 8\n"
     ]
    }
   ],
   "source": [
    "emb = nnModel.get_embeddings(index2word, VOCAB_SIZE, EMB_SIZE)\n",
    "model = nnModel.build_model(emb,MAX_CAPTION_LEN, VOCAB_SIZE, EMB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "repeatvector_9 (RepeatVector)    (None, 15, 4096)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 15, 300)       254700                                       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 15, 300)       1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 15, 300)       254700                                       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 15, 300)       1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 15, 1024)      17574912    merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 15, 1024)      0           gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 15, 1024)      6294528     dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 15, 849)       870225      gru_2[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 25,251,465\n",
      "Trainable params: 25,250,265\n",
      "Non-trainable params: 1,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "                    nnModel.generate_arrays_from_file(train_img_vgg_path,train_indexed_captions_path,\n",
    "                                              train_future_words_path,train_prev_captions_path),\n",
    "                    samples_per_epoch = 219136,\n",
    "                    nb_epoch=1,\n",
    "                    validation_data = nnModel.generate_arrays_from_file(test_img_vgg_path,test_indexed_captions_path,\n",
    "                                                   test_future_words_path,test_prev_captions_path),\n",
    "                    nb_val_samples = 2048,\n",
    "                    callbacks=[#callbacks.RemoteMonitor(),\n",
    "                               callbacks.CSVLogger(\"./training.log\")]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(nnModel.generate_arrays_from_file(test_img_vgg_path,test_indexed_captions_path,\n",
    "                                                   test_future_words_path,test_prev_captions_path),\n",
    "                         val_samples = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.save_weights(save_path + models_folder+\"big/\" +'app_100_length_15_past_word_30_epoch_300d_gru_2x1048_big.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(save_path + models_folder+\"big/\" +'app_100_length_15_past_word_20_epoch_300d_gru_2x1048_big.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_arrays_from_file(train_img_vgg_path,\n",
    "                                                   train_indexed_captions_path,\n",
    "                                                   train_future_words_path,\n",
    "                                                   train_prev_captions_path),\n",
    "                        val_samples = NR_TRAIN_EXAMPLES,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_arrays_from_file(test_img_vgg_path,\n",
    "                                                   test_indexed_captions_path,\n",
    "                                                   test_future_words_path,\n",
    "                                                   test_prev_captions_path),\n",
    "                        val_samples = NR_TEST_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_caption_list(captions_raw):\n",
    "    \n",
    "    stripped_caption_list = []\n",
    "    caption_list = list(captions_raw)\n",
    "    for caption in caption_list:\n",
    "        stripped = str(caption).split(\" \")[1:-1]\n",
    "        stripped_caption_list.append(\" \".join(stripped))\n",
    "        \n",
    "    return stripped_caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_start = 500\n",
    "nr_images = 200\n",
    "\n",
    "# images_concat_t = train_images_concat_t #ATENTIE\n",
    "# real_captions = train_captions_raw\n",
    "\n",
    "images_concat_t = test_images_concat_t\n",
    "real_captions = strip_caption_list(test_captions_raw)\n",
    "\n",
    "print(images_concat_t.shape)\n",
    "\n",
    "(images,predicted_captions) = make_prediction_on_dataset(images_concat_t,window_start,nr_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_captions = [str(a) +\" ------ \" + str(b)  for a,b in zip(predicted_captions,real_captions)]\n",
    "# merged_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preproc.plot_predictions(images,titles = predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words2app = preproc.most_common_words(predicted_captions,500)\n",
    "common_words2app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searched_word = \"bus\"\n",
    "(found_images,found_captions) = search_images_by(searched_word,images,predicted_captions)\n",
    "print(\"Number of results = %d\"%len(found_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.plot_predictions(found_images,found_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unet(img_rows, img_cols,nr_labels):\n",
    "    \n",
    "    inputs = Input((3, img_rows, img_cols))\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(bn)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(nr_labels, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_2_supercat_dict = load_obj(\"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/detection/misc/cat_2_supercat_dict.pkl\")\n",
    "supercat2indexpath = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/detection/misc/cat_2_index_dict_person_animal_vehicle_10e.pkl\"\n",
    "supercat_2_index_dict = load_obj(supercat2indexpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_2_supercat_dict = dict([(vv,kk) for kk,vv in supercat_2_index_dict.iteritems()])\n",
    "\n",
    "print(supercat_2_index_dict)\n",
    "print(index_2_supercat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def can_generate_segmentation(caption):\n",
    "    caption_words = caption.split(\" \")\n",
    "    match_word_arr = [word in cat_2_supercat_dict for word in caption_words]\n",
    "    found = np.any(match_word_arr)\n",
    "    \n",
    "    if(not found):\n",
    "        return (False,-1)\n",
    "    \n",
    "    word_categories = np.array(caption_words)[np.array(match_word_arr)]\n",
    "                                           \n",
    "    if(len(word_categories) == 0):\n",
    "        return (False,-1)\n",
    "    \n",
    "    category = word_categories[0]\n",
    "   \n",
    "    if(category not in cat_2_supercat_dict):\n",
    "        return (False,-1)\n",
    "\n",
    "    supercategory = cat_2_supercat_dict[category]\n",
    "\n",
    "    if(supercategory not in supercat_2_index_dict):\n",
    "        return (False,-1)\n",
    "        \n",
    "    return (True,supercat_2_index_dict[supercategory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_net_model = get_unet(224,224,nr_labels = 3)\n",
    "save_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/detection/models/\"\n",
    "seg_net_model.load_weights(save_path+\"person_animal_vehicle_10e.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_mask_to_supercat_list = [can_generate_segmentation(caption) for caption in predicted_captions]\n",
    "seg_mask = [mask for (mask,_) in seg_mask_to_supercat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_captions = np.asarray(predicted_captions)[np.asarray(seg_mask)]\n",
    "seg_images = np.asarray(images)[np.asarray(seg_mask)]\n",
    "image_supercat_list = [supercat for (_,supercat) in np.asarray(seg_mask_to_supercat_list)[np.asarray(seg_mask)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(seg_captions[i])\n",
    "    print(\"\\t ==>\" + index_2_supercat_dict[image_supercat_list[i]])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_images_np = np.stack(seg_images)\n",
    "transposed_img = np.transpose(seg_images_np,(0,3,1,2))\n",
    "transposed_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = seg_net_model.predict(transposed_img)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_predictions = []\n",
    "for index in range(len(predictions)):\n",
    "    s_index = image_supercat_list[index]\n",
    "    current_prediction = predictions[index]\n",
    "    category_predictions.append(current_prediction[s_index:s_index+1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotImageCaptionToMask(images, predictions, titles):\n",
    "  \n",
    "    nr_rows = len(predictions)\n",
    "    \n",
    "    figsize = 8\n",
    "    plt.figure(figsize=(figsize,figsize))\n",
    "    for index in range(nr_rows):\n",
    "   \n",
    "        squeezed_pred = np.squeeze(predictions[index])\n",
    "        \n",
    "        image = images[index]\n",
    "        \n",
    "        s_index = image_supercat_list[index]\n",
    "        supercategory = index_2_supercat_dict[s_index]\n",
    "        \n",
    "        \n",
    "        nr_cols = 2\n",
    "        \n",
    "        plt.subplot(1,nr_cols,1)\n",
    "        plt.title(titles[index] + \" => \" + supercategory)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        \n",
    "        \n",
    "        plt.subplot(1,nr_cols,2)\n",
    "\n",
    "        plt.imshow(squeezed_pred)\n",
    "        plt.imshow(image, alpha=0.5)\n",
    "    \n",
    "        \n",
    "        plt.figure(figsize=(figsize,figsize))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotImageCaptionToMask(seg_images, category_predictions,seg_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = seg_net_model.predict(video_frames)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animal_index = supercat_2_index_dict['person']\n",
    "animal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animal_preds = predictions[:,animal_index:animal_index+1,:,:]\n",
    "animal_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transposed_animal_preds = np.squeeze(animal_preds)\n",
    "transposed_animal_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_movie_mp4(video_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_movie_mp4(transposed_animal_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Video Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_concat_video(video_paths):\n",
    "    list_of_video_frames = [ get_mp4_vid_frames(video_base_path + video_path + \".mp4\") for video_path in video_paths]\n",
    "    video_frame_list = [y for x in list_of_video_frames for y in x]\n",
    "    input_video_frames = np.transpose(np.asarray(video_frame_list),(0,3,1,2))\n",
    "    return input_video_frames\n",
    "\n",
    "\n",
    "def plot_index_arr(video_frames,found_indexes):\n",
    "    index_arr = get_index_arr(video_frames,found_indexes)\n",
    "    plt.figure()\n",
    "    ind = np.asarray(range(len(index_arr)))\n",
    "    plt.bar(ind, index_arr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_base_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/video/\"\n",
    "video_paths = [\"horse\",\"boat\",\"baseball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_video_frames = get_concat_video(video_paths)\n",
    "print(input_video_frames.shape)\n",
    "\n",
    "(video_frames,video_frames_captions) = make_prediction_on_dataset(input_video_frames,0,len(input_video_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## See most common objects in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_words2app = preproc.most_common_words(video_frames_captions,50)\n",
    "common_words2app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_words = [word for (word,count) in common_words2app[:4]]\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_arr(video_frames,found_indexes):\n",
    "    index_arr = np.zeros(len(video_frames))\n",
    "    for found_index in found_indexes:\n",
    "        index_arr[found_index] = 1\n",
    "        \n",
    "    return index_arr\n",
    "    \n",
    "def get_found_index_arr(video_search_word,video_frames,video_frames_captions):\n",
    "    (_,_,found_indexes) = search_video_by(video_search_word,video_frames,video_frames_captions)\n",
    "    return get_index_arr(video_frames,found_indexes)\n",
    "    return found_indexes\n",
    "\n",
    "def plot_bar_chart(word_indexes_arr,most_common_words):\n",
    "    index = range(len(word_indexes_arr[0]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(index, word_indexes_arr[0],\n",
    "                     color='r',\n",
    "                     label=most_common_words[0])\n",
    "\n",
    "    plt.bar(index, word_indexes_arr[1],\n",
    "                     color='b',\n",
    "                     label=most_common_words[1])\n",
    "\n",
    "    plt.bar(index, word_indexes_arr[2],\n",
    "                     color='g',\n",
    "                     label=most_common_words[2])\n",
    "\n",
    "    plt.bar(index, word_indexes_arr[3],\n",
    "                     color='y',\n",
    "                     label=most_common_words[3])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_frame_occurences(most_common_words,video_frames,video_frames_captions):\n",
    "    word_indexes_arr = [get_found_index_arr(word,video_frames,video_frames_captions) for word in most_common_words]\n",
    "    plot_bar_chart(word_indexes_arr,most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_frame_occurences(most_common_words,video_frames,video_frames_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on misc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images_path = save_path + misc_images_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images = []\n",
    "for img_path in os.listdir(misc_images_path):\n",
    "    img = PIL.Image.open(misc_images_path+img_path)\n",
    "    img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "    img = np.asarray(img)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    misc_images.append(img)\n",
    "    \n",
    "stacked_images = np.vstack(misc_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(misc_images,misc_predicted_captions) = make_prediction_on_dataset(stacked_images)\n",
    "preproc.plot_predictions(misc_images,misc_predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare data for \"Caption Turing Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_captions(image_data_arr):\n",
    "    caption_bucket_list = preproc.get_captions_list(image_data_arr)\n",
    "    caption_bucket_length = len(caption_bucket_list[0])\n",
    "    \n",
    "    captions = np.stack(caption_bucket_list)\n",
    "    \n",
    "    print(\"caption_bucket_length = %d\"%caption_bucket_length)\n",
    "    print(\"captions.shape = %s\"%str(captions.shape))\n",
    "    \n",
    "    return (caption_bucket_list)\n",
    "\n",
    "\n",
    "def get_imgs_and_captions(base_images_path,base_annotation_path,nr_items):\n",
    "    image_data_arr = preproc.get_image_data_arr(base_images_path,base_annotation_path,nr_items)\n",
    "    \n",
    "    captions= get_all_captions(image_data_arr)\n",
    "    images = preproc.construct_images_concat_t(image_data_arr)\n",
    "    \n",
    "    print(\"images.shape = %s\"%str(images.shape))\n",
    "    return (images,captions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_images_path = val_images_path\n",
    "base_annotation_path = val_annotation_path\n",
    "\n",
    "(images,caption_bucket_list) = get_imgs_and_captions(base_images_path,base_annotation_path,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(_,predicted_captions) = make_prediction_on_dataset(images,0,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_captions_1 = strip_caption_list(caption_bucket_list[0])\n",
    "real_captions_2 = strip_caption_list(caption_bucket_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_df = percentile_list = pd.DataFrame(\n",
    "    {'image': list(images),\n",
    "     'predicted_caption': predicted_captions,\n",
    "     'real_caption_1': real_captions_1,\n",
    "     'real_caption_2': real_captions_2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
