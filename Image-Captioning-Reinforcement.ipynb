{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge,BatchNormalization,Input\n",
    "from keras.preprocessing import sequence\n",
    "from keras import callbacks\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from numpy.random import random, permutation, randn, normal \n",
    "\n",
    "import os\n",
    "\n",
    "import preprocessing as preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_images_by(searched_word,images,predicted_captions):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemm_word = lmtzr.lemmatize(searched_word)\n",
    "    \n",
    "    found_indexes = []\n",
    "    for index,caption in enumerate(predicted_captions):\n",
    "        lemm_caption_words = [lmtzr.lemmatize(word) for word in caption.split()]\n",
    "        if lemm_word in lemm_caption_words:\n",
    "            found_indexes.append(index)\n",
    "    \n",
    "    return ([images[i] for i in found_indexes],[predicted_captions[i] for i in found_indexes])\n",
    "\n",
    "def make_prediction_on_dataset(images_concat_t, window_start = None, no_images = None):\n",
    "    \n",
    "    if(window_start == None):\n",
    "        window_start = 0\n",
    "        \n",
    "    if(no_images == None):\n",
    "        no_images = len(images_concat_t)\n",
    "    \n",
    "    vgg_model = get_vgg_model()\n",
    "    \n",
    "    images2Captions = [make_prediction(i,images_concat_t,vgg_model) for i in tqdm(range(window_start,window_start+no_images))]\n",
    "    images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "    predicted_captions = [image2Caption[1] for image2Caption in images2Captions]\n",
    "\n",
    "    \n",
    "    return (images,predicted_captions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = app_3_length_15_data_path\n",
    "\n",
    "train_path = base_path + train_folder\n",
    "val_path = base_path + val_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "test_images_concat_t = preproc.get_images_concat(val_path + images_concat_folder+ 'images_concat.bc',1000)\n",
    "print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load precomputed misc data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_path = app_3_length_15_data_path\n",
    "\n",
    "train_img_vgg_path = base_path + train_folder + batch_folder + images_vgg_4096_folder\n",
    "train_indexed_captions_path = base_path + train_folder + batch_folder + indexed_captions_folder\n",
    "train_raw_captions_path = base_path + train_folder + batch_folder + captions_folder\n",
    "train_future_words_path = base_path + train_folder + batch_folder + indexed_future_words_folder\n",
    "\n",
    "test_img_vgg_path = base_path + val_folder + batch_folder +images_vgg_4096_folder\n",
    "test_indexed_captions_path = base_path + val_folder + batch_folder + indexed_captions_folder\n",
    "test_raw_captions_path = base_path + val_folder + batch_folder+captions_folder\n",
    "test_future_words_path = base_path + val_folder + batch_folder+indexed_future_words_folder\n",
    "\n",
    "indexed_prev_captions_folder = \"indexed-prev-captions/\"\n",
    "train_prev_captions_path = base_path + train_folder + batch_folder + indexed_prev_captions_folder\n",
    "test_prev_captions_path = base_path + val_folder + batch_folder + indexed_prev_captions_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 683.40it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 668.03it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_words = preproc.load_obj(base_path + general_datastruct_folder+\"unique_words\")\n",
    "word2index = preproc.load_obj(base_path+general_datastruct_folder+\"word2index\")\n",
    "index2word = preproc.load_obj(base_path+general_datastruct_folder+\"index2word\")\n",
    "\n",
    "(train_captions_raw,_) = preproc.get_captions_raw_and_indexed(train_raw_captions_path,train_indexed_captions_path)\n",
    "(test_captions_raw,_) = preproc.get_captions_raw_and_indexed(test_raw_captions_path,test_indexed_captions_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_words)\n",
    "MAX_CAPTION_LEN = 15 # ATENTIE AICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_CAPTION_LEN = 15\n",
      "VOCAB_SIZE = 7275\n"
     ]
    }
   ],
   "source": [
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found = 7025\n",
      "Not found = 249\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 200\n",
    "vecs, words, wordidx = preproc.load_vectors(save_path+glove_folder+\"6B.\"+str(EMB_SIZE)+\"d\")\n",
    "\n",
    "emb = preproc.create_emb(vecs, words, wordidx,index2word,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VGG\n",
    "\n",
    "def get_vgg_model():\n",
    "    image_model = Vgg16().model\n",
    "    image_model.pop()\n",
    "    image_model.pop()\n",
    "    image_model.trainable = False\n",
    "    image_model.add(RepeatVector(MAX_CAPTION_LEN))\n",
    "    return image_model\n",
    "\n",
    "def get_precomputed_input_model():\n",
    "    input_model = Sequential()\n",
    "    input_model.add(RepeatVector(MAX_CAPTION_LEN,input_shape=(4096,)))\n",
    "    return input_model\n",
    "\n",
    "# GRU\n",
    "\n",
    "def get_language_model(emb):\n",
    "    language_model = Sequential()\n",
    "    language_model.add(Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_CAPTION_LEN,weights=[emb]))\n",
    "    Dropout(0.2)\n",
    "    language_model.add(BatchNormalization())\n",
    "    return language_model\n",
    "\n",
    "def get_reinforcement_model():\n",
    "    reinforcement_model = Sequential()\n",
    "    reinforcement_model.add(Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_CAPTION_LEN,weights=[emb]))\n",
    "    Dropout(0.2)\n",
    "    reinforcement_model.add(BatchNormalization())\n",
    "    return reinforcement_model\n",
    "\n",
    "# Top level model\n",
    "\n",
    "def build_model(image_model,language_model,reinforcement_model):\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, language_model,reinforcement_model], mode='concat'))\n",
    "    model.add(GRU(1024, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE, activation = 'softmax')))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_model = get_precomputed_input_model()\n",
    "language_model = get_language_model(emb)\n",
    "reinforcement_model = get_reinforcement_model()\n",
    "model = build_model(image_model,language_model,reinforcement_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data(img_vgg_path,indexed_captions_path,future_words_path,):\n",
    "    img_vgg_elements = os.listdir(img_vgg_path)\n",
    "    indexed_captions_elements = os.listdir(indexed_captions_path)\n",
    "    future_words_elements = os.listdir(future_words_path)\n",
    "\n",
    "    img_vgg_elements.sort()\n",
    "    indexed_captions_elements.sort()\n",
    "    future_words_elements.sort()\n",
    "\n",
    "    nr_batches = 1\n",
    "\n",
    "    for index in tqdm(range(nr_batches)):\n",
    "\n",
    "        img_vgg_batch_list = []\n",
    "        indexed_caption_batch_list = []\n",
    "        future_words_batch_list = []\n",
    "\n",
    "        img_vgg_el_name = img_vgg_elements[index]\n",
    "        indexed_caption_name = indexed_captions_elements[index]\n",
    "        future_words_el_name = future_words_elements[index]\n",
    "\n",
    "        img_vgg = preproc.load_array(img_vgg_path+\"/\"+img_vgg_el_name)\n",
    "        indexed_caption = preproc.load_array(indexed_captions_path+\"/\"+indexed_caption_name)\n",
    "        future_words = preproc.load_array(future_words_path+\"/\"+future_words_el_name)\n",
    "\n",
    "        img_vgg_batch_list.append(img_vgg)\n",
    "        indexed_caption_batch_list.append(indexed_caption)\n",
    "        future_words_batch_list.append(future_words)\n",
    "\n",
    "    img_vgg_big = np.vstack(img_vgg_batch_list)\n",
    "    indexed_caption_big = np.vstack(indexed_caption_batch_list)\n",
    "    future_words_big = np.vstack(future_words_batch_list)\n",
    "\n",
    "    print(img_vgg_big.shape)\n",
    "    print(indexed_caption_big.shape)\n",
    "    print(future_words_big.shape)\n",
    "\n",
    "    return img_vgg_big, indexed_caption_big, future_words_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_arrays_from_file(img_vgg_path,indexed_captions_path,future_words_path,current_words_path):\n",
    "    while 1:\n",
    "        img_vgg_elements = os.listdir(img_vgg_path)\n",
    "        indexed_captions_elements = os.listdir(indexed_captions_path)\n",
    "        future_words_elements = os.listdir(future_words_path)\n",
    "        current_words_elements = os.listdir(current_words_path)\n",
    "        \n",
    "        img_vgg_elements.sort()\n",
    "        indexed_captions_elements.sort()\n",
    "        future_words_elements.sort()\n",
    "        current_words_elements.sort()\n",
    "\n",
    "        nr_elem = len(img_vgg_elements)\n",
    "        \n",
    "        BATCH_SIZE = 1\n",
    "        \n",
    "        for index in range(nr_elem/BATCH_SIZE):\n",
    "            \n",
    "            img_vgg_batch_list = []\n",
    "            indexed_caption_batch_list = []\n",
    "            future_words_batch_list = []\n",
    "            current_words_batch_list = []\n",
    "            \n",
    "            for elem_in_batch in range(BATCH_SIZE):\n",
    "                \n",
    "                img_vgg_el_name = img_vgg_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "                indexed_caption_name = indexed_captions_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "                future_words_el_name = future_words_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "                current_words_el_name = current_words_elements[index*BATCH_SIZE + elem_in_batch]\n",
    "\n",
    "                img_vgg = preproc.load_array(img_vgg_path+\"/\"+img_vgg_el_name)\n",
    "                indexed_caption = preproc.load_array(indexed_captions_path+\"/\"+indexed_caption_name)\n",
    "                future_words = preproc.load_array(future_words_path+\"/\"+future_words_el_name)\n",
    "                current_words = preproc.load_array(current_words_path+\"/\"+current_words_el_name)\n",
    "                \n",
    "                img_vgg_batch_list.append(img_vgg)\n",
    "                indexed_caption_batch_list.append(indexed_caption)\n",
    "                future_words_batch_list.append(future_words)\n",
    "                current_words_batch_list.append(current_words)\n",
    "                \n",
    "            img_vgg_big = np.vstack(img_vgg_batch_list)\n",
    "            indexed_caption_big = np.vstack(indexed_caption_batch_list)\n",
    "            future_words_big = np.vstack(future_words_batch_list)\n",
    "            current_words_big = np.vstack(current_words_batch_list)\n",
    "            \n",
    "#             print(img_vgg_big.shape)\n",
    "#             print(indexed_caption_big.shape)\n",
    "#             print(future_words_big.shape)\n",
    "    \n",
    "            yield ([img_vgg_big,indexed_caption_big,current_words_big], future_words_big)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_img_vgg, test_indexed_captions, test_future_words = get_test_data(test_img_vgg_path,\n",
    "#                                                                        test_indexed_captions_path,\n",
    "#                                                                        test_future_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "                    generate_arrays_from_file(train_img_vgg_path,train_indexed_captions_path,\n",
    "                                              train_future_words_path,train_prev_captions_path),\n",
    "                    samples_per_epoch=82000,\n",
    "                    nb_epoch=10\n",
    "#                     validation_data = ([test_img_vgg, test_indexed_captions], test_future_words),\n",
    "#                     callbacks=[callbacks.RemoteMonitor()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.plot_loss_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_arrays_from_file(test_img_vgg_path,test_indexed_captions_path,\n",
    "                                                   test_future_words_path,test_prev_captions_path),\n",
    "                         val_samples = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(save_path + models_folder +'app_3_length_15_past_word_65_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.load_weights(save_path + models_folder +'app_3_length_15_30_epoch.h5')\n",
    "model.load_weights(save_path + models_folder +'app_3_length_15_past_word_65_epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction(random_number,images_concat_t,vgg_model):\n",
    "    startIndex = word2index[\"START\"]\n",
    "    start_captions = [[startIndex]]\n",
    "    start_captions = sequence.pad_sequences(start_captions, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "\n",
    "    firstImage = np.expand_dims(images_concat_t[random_number], axis=0)\n",
    "    first_image_vgg_features = vgg_model.predict(firstImage)\n",
    "    first_image_input = np.squeeze(first_image_vgg_features)[0].reshape(1,4096)\n",
    "    \n",
    "    firstCaption = np.expand_dims(start_captions[0], axis=0) \n",
    "    prev_word_indexed_captions = np.expand_dims(list(start_captions[0]), axis=0)\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    endGenerated = False\n",
    "    i = 0\n",
    "        \n",
    "    while ((not endGenerated) & (i < MAX_CAPTION_LEN-1)):\n",
    "       \n",
    "        predictions = model.predict([first_image_input, firstCaption, prev_word_indexed_captions])\n",
    "        predictions = predictions[0]\n",
    "        \n",
    "        currentPred = predictions[i]\n",
    "        \n",
    "        max_index = np.argmax(currentPred)\n",
    "        \n",
    "        outputs.append(max_index)\n",
    "        firstCaption[0,i+1] = max_index\n",
    "        \n",
    "        prev_word_indexed_captions[0,i+1] = firstCaption[0,i]\n",
    "                \n",
    "        i+=1\n",
    "\n",
    "        if(index2word[max_index] == \"END\"):\n",
    "            endGenerated = True\n",
    "\n",
    "    caption = ' '.join([index2word[x] for x in firstCaption[0][:i+1]])\n",
    "\n",
    "    drawImage = firstImage[0]\n",
    "    drawImageT = np.transpose(drawImage,(1,2,0))\n",
    "    plt.imshow(drawImageT)\n",
    "    \n",
    "    return (drawImageT,caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:14<00:00,  9.01it/s]\n"
     ]
    }
   ],
   "source": [
    "window_start = 0\n",
    "nr_images = 128\n",
    "\n",
    "# images_concat_t = train_images_concat_t\n",
    "# real_captions = train_captions_raw\n",
    "\n",
    "images_concat_t = test_images_concat_t\n",
    "real_captions = test_captions_raw\n",
    "\n",
    "(images,predicted_captions) = make_prediction_on_dataset(images_concat_t,window_start,nr_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'START A wooden table topped with lots of bottles and a small wooden table END',\n",
       " u'START A snowboarder is going down the snow covered slope END',\n",
       " u'START A woman sitting on a chair while holding a remote END',\n",
       " u'START A living room with wooden floors and a TV END',\n",
       " u'START A man and a skateboard at the park END',\n",
       " u'START A kitchen with a large white stove top oven END',\n",
       " u'START A teddy bear sitting on top of a wooden shelf END',\n",
       " u'START A double decker bus driving down a street END',\n",
       " u'START Two men stand on a market display of various fruits END',\n",
       " u'START A bathroom with a white toilet and a sink END',\n",
       " u'START A zebra that is in the grass on a field END',\n",
       " u'START A window sits on a wooden bench overlooking a small window END',\n",
       " u'START A woman is standing up to the child in the kitchen END',\n",
       " u'START A man is standing in the dark looking at night END',\n",
       " u'START A soccer player is in the grass playing soccer END',\n",
       " u'START A picture of a person in a hat hat END',\n",
       " u'START A man is walking along a beach holding an umbrella END',\n",
       " u'START A man riding a motorcycle with a back of a street END',\n",
       " u'START A group of people playing a game of frisbee END',\n",
       " u'START A desk with a computer and a computer monitor END',\n",
       " u'START A man holding a glass of wine glasses END',\n",
       " u'START a herd of elephants walking across a river END',\n",
       " u'START A large white jetliner sitting on top of an airport runway END',\n",
       " u'START A dirty bathroom with a dirty sink next to a toilet END',\n",
       " u'START A herd of zebra grazing on top of a lush green field END',\n",
       " u'START A room with a large window and a sink in a room END',\n",
       " u'START A couple of men standing next to each other END',\n",
       " u'START A fire hydrant sitting in the middle of a street END',\n",
       " u'START A clock tower sitting on top of a building END',\n",
       " u'START A young boy is eating a piece of food END',\n",
       " u'START A person on a field with a Frisbee in the grass END',\n",
       " u'START A man on a skateboard next to a building END',\n",
       " u'START A man riding a wave on top of a surfboard END',\n",
       " u'START A black and white cow standing in a grassy field END',\n",
       " u'START A elephant is standing in the sun. END',\n",
       " u'START A man is surfing on a wave in the ocean END',\n",
       " u'START A pizza sitting on top of a pan in the oven END',\n",
       " u'START A person is holding a cell phone in the left hand END',\n",
       " u'START A group of school buses are parked in a parking lot END',\n",
       " u'START A woman is standing on a tennis court holding a racquet END',\n",
       " u'START A bathroom with a white bath tub next to a toilet END',\n",
       " u'START A large white bed sitting next to a fire hydrant END',\n",
       " u'START A room with a television and a television in the room. END',\n",
       " u'START A man flying through the air while riding a snowboard END',\n",
       " u'START A double decker bus at a stop sign with street signs END',\n",
       " u'START A black and white dog laying on top of a bed END',\n",
       " u'START A group of people that are sitting on a street END',\n",
       " u'START A kitchen with wooden floors and a stove top oven END',\n",
       " u'START A girl is playing tennis on a cour END',\n",
       " u'START A motorcycle parked in a parking lot next to a parking lot END',\n",
       " u'START A group of people walking down a street holding umbrellas END',\n",
       " u'START A living room with a couch sitting in a living room END',\n",
       " u'START A man riding a snowboard down a snow covered slope END',\n",
       " u'START A train pulling into a train station for passengers at a train station END',\n",
       " u'START A stop sign is on the side of the road END',\n",
       " u'START A baseball player standing next to home plate END',\n",
       " u'START A bus driving down a street next to a building END',\n",
       " u'START A herd of elephants walking across a grass covered field END',\n",
       " u'START A baseball player swinging a bat over home plate END',\n",
       " u'START A woman sitting at a table with bottles and wine END',\n",
       " u'START A dog that is standing in the grass END',\n",
       " u'START A pizza pan that is on a plate of food END',\n",
       " u'START A plate of food that is on a table END',\n",
       " u'START A man and woman are playing a video game END',\n",
       " u'START A bathroom with a sink and a mirror in a tiled wall END',\n",
       " u'START A cat standing on top of a counter top END',\n",
       " u'START A sandwich and a plate with a sandwich on a table END',\n",
       " u'START A man sitting at a table while eating a cake END',\n",
       " u'START A bunch of green bananas sitting on top of a table. END',\n",
       " u'START A person standing next to a fire hydrant in a street END',\n",
       " u'START A man flying a kite on top of a sandy beach END',\n",
       " u'START A flock of birds flying over a mountain top END',\n",
       " u'START A horse standing on top of a lush green field END',\n",
       " u'START A woman walking past a store with a cell phone END',\n",
       " u'START A train engine is pulling into a train station END',\n",
       " u'START a couple of elephants walking out of a field of gras END',\n",
       " u'START A girl holds an umbrella while standing next to a child END',\n",
       " u'START A little girl cutting a piece of cake on top of a table END',\n",
       " u'START A table topped with a banana and other cup END',\n",
       " u'START A tall building with a clock tower sticking out of it END',\n",
       " u'START A group of people riding bikes down a street END',\n",
       " u'START A baseball player swinging a bat on a field END',\n",
       " u'START A bathroom with a sink and tub next to a tub END',\n",
       " u'START A market filled with lots of vegetables and fruits END',\n",
       " u'START A person that is on a plate with a cake END',\n",
       " u'START A motorcycle with a parked car sitting on the street END',\n",
       " u'START A woman standing on a tennis court holding a racquet END',\n",
       " u'START A bathroom with a white toilet and a microwave END',\n",
       " u'START a man sitting at a table with a plate with food END',\n",
       " u'START A laptop computer sitting on top of a wooden table END',\n",
       " u'START A fighter jet sitting on top of an airport tarmac END',\n",
       " u'START A man holding a tennis racquet on top of a tennis court END',\n",
       " u'START A computer desk with a keyboard and a monitor END',\n",
       " u'START A teddy bear is laying on a bed END',\n",
       " u'START A woman is talking on a phone with an open umbrella END',\n",
       " u'START A cow is walking along the river by the water END',\n",
       " u'START A group of people laying on the beach under umbrellas END',\n",
       " u'START A woman sits at a table using laptop computers END',\n",
       " u'START A man riding skis down a ski slope END',\n",
       " u'START A girl sitting in a bed with a stuffed animal END',\n",
       " u'START A couple of zebra standing next to each other on a dirt road END',\n",
       " u'START A bus that is driving down the street END',\n",
       " u'START A bed with a stuffed animal laying on top of bed END',\n",
       " u'START A girl brushing her teeth in front of a bathroom END',\n",
       " u'START A woman sitting at a table with bottles of wine END',\n",
       " u'START A cat standing on a dirty looking of a cat END',\n",
       " u'START A kitten is looking at a laptop computer END',\n",
       " u'START A dog is lying on a bed looking at a train END',\n",
       " u'START A living room filled with a wooden table with vases and other END',\n",
       " u'START A desk with a computer and a keyboard and mouse END',\n",
       " u'START A man holding a phone while standing in a yard END',\n",
       " u'START A bed with a pair of legs and a television END',\n",
       " u'START A man riding a skateboard in front of a building END',\n",
       " u'START A bed sitting in a chair in a room END',\n",
       " u'START A man riding a snowboard on snow covered slope END',\n",
       " u'START A man sitting at a table with laptop computer END',\n",
       " u'START A train is pulling into a train station END',\n",
       " u'START A child climbing into the refrigerator looking into the refrigerator END',\n",
       " u'START A pizza sitting on top of a white plate END',\n",
       " u'START A cat laying on the floor next to a cat END',\n",
       " u'START A clock sitting on top of a wooden table END',\n",
       " u'START a stop sign that is attached to a pole END',\n",
       " u'START A man sitting on a bench near a bench in the ocean END',\n",
       " u'START A black and white photo of a cat playing with a hat END',\n",
       " u'START A man riding a surfboard on top of a wave END',\n",
       " u'START A living room filled with furniture and a fire place END',\n",
       " u'START A flock of birds walking across a beach near the ocean END',\n",
       " u'START A room filled with lots of furniture and a flat screen TV END']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.save_obj(predicted_captions,val_path+\"predictions/\"+\"predicted_captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preproc.plot_predictions(images,titles = predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def most_common_words(captions,word_limit):\n",
    "    \n",
    "#     sentences = [caption.split() for caption in captions]\n",
    "#     words = []\n",
    "#     for word in sentences:\n",
    "#         words.extend(word)\n",
    "\n",
    "#     counter=collections.Counter(words)\n",
    "#     return counter.most_common(word_limit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'preprocessing.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'sitting', 23),\n",
       " (u'top', 23),\n",
       " (u'man', 22),\n",
       " (u'standing', 16),\n",
       " (u'table', 15),\n",
       " (u'next', 13),\n",
       " (u'street', 11),\n",
       " (u'woman', 10),\n",
       " (u'white', 10),\n",
       " (u'room', 10),\n",
       " (u'wooden', 9),\n",
       " (u'riding', 9),\n",
       " (u'bed', 9),\n",
       " (u'holding', 9),\n",
       " (u'walking', 8),\n",
       " (u'plate', 8),\n",
       " (u'train', 8),\n",
       " (u'field', 8),\n",
       " (u'bathroom', 7),\n",
       " (u'computer', 7),\n",
       " (u'group', 6),\n",
       " (u'cat', 6),\n",
       " (u'laying', 5),\n",
       " (u'people', 5),\n",
       " (u'girl', 5),\n",
       " (u'living', 5),\n",
       " (u'looking', 5),\n",
       " (u'sink', 5),\n",
       " (u'tennis', 5),\n",
       " (u'building', 5),\n",
       " (u'grass', 5),\n",
       " (u'person', 5),\n",
       " (u'playing', 5),\n",
       " (u'slope', 4),\n",
       " (u'covered', 4),\n",
       " (u'food', 4),\n",
       " (u'large', 4),\n",
       " (u'filled', 4),\n",
       " (u'station', 4),\n",
       " (u'fire', 4),\n",
       " (u'bus', 4),\n",
       " (u'laptop', 4),\n",
       " (u'toilet', 4),\n",
       " (u'player', 4),\n",
       " (u'beach', 4),\n",
       " (u'phone', 4),\n",
       " (u'motorcycle', 3),\n",
       " (u'oven', 3),\n",
       " (u'bench', 3),\n",
       " (u'window', 3),\n",
       " (u'black', 3),\n",
       " (u'hat', 3),\n",
       " (u'couple', 3),\n",
       " (u'stop', 3),\n",
       " (u'wave', 3),\n",
       " (u'desk', 3),\n",
       " (u'parked', 3),\n",
       " (u'sign', 3),\n",
       " (u'zebra', 3),\n",
       " (u'parking', 3),\n",
       " (u'umbrella', 3),\n",
       " (u'driving', 3),\n",
       " (u'clock', 3),\n",
       " (u'lots', 3),\n",
       " (u'across', 3),\n",
       " (u'racquet', 3),\n",
       " (u'herd', 3),\n",
       " (u'baseball', 3),\n",
       " (u'kitchen', 3),\n",
       " (u'skateboard', 3),\n",
       " (u'flying', 3),\n",
       " (u'snowboard', 3),\n",
       " (u'hydrant', 3),\n",
       " (u'elephants', 3),\n",
       " (u'lot', 3),\n",
       " (u'child', 3),\n",
       " (u'television', 3),\n",
       " (u'dirty', 3),\n",
       " (u'cake', 3),\n",
       " (u'pulling', 3),\n",
       " (u'bottles', 3),\n",
       " (u'snow', 3),\n",
       " (u'pizza', 3),\n",
       " (u'court', 3),\n",
       " (u'dog', 3),\n",
       " (u'ocean', 3),\n",
       " (u'green', 3),\n",
       " (u'tub', 3),\n",
       " (u'wine', 3),\n",
       " (u'stove', 2),\n",
       " (u'chair', 2),\n",
       " (u'birds', 2),\n",
       " (u'cell', 2),\n",
       " (u'soccer', 2),\n",
       " (u'animal', 2),\n",
       " (u'bat', 2),\n",
       " (u'bear', 2),\n",
       " (u'game', 2),\n",
       " (u'front', 2),\n",
       " (u'furniture', 2),\n",
       " (u'sandwich', 2),\n",
       " (u'small', 2),\n",
       " (u'tower', 2),\n",
       " (u'river', 2),\n",
       " (u'keyboard', 2),\n",
       " (u'monitor', 2),\n",
       " (u'umbrellas', 2),\n",
       " (u'men', 2),\n",
       " (u'along', 2),\n",
       " (u'piece', 2),\n",
       " (u'cow', 2),\n",
       " (u'airport', 2),\n",
       " (u'road', 2),\n",
       " (u'floors', 2),\n",
       " (u'market', 2),\n",
       " (u'teddy', 2),\n",
       " (u'stuffed', 2),\n",
       " (u'double', 2),\n",
       " (u'decker', 2),\n",
       " (u'fruits', 2),\n",
       " (u'lush', 2),\n",
       " (u'swinging', 2),\n",
       " (u'sits', 2),\n",
       " (u'flock', 2),\n",
       " (u'TV', 2),\n",
       " (u'topped', 2),\n",
       " (u'pan', 2),\n",
       " (u'refrigerator', 2),\n",
       " (u'surfboard', 2),\n",
       " (u'home', 2),\n",
       " (u'eating', 2),\n",
       " (u'near', 2),\n",
       " (u'cour', 1),\n",
       " (u'bananas', 1),\n",
       " (u'photo', 1),\n",
       " (u'kitten', 1),\n",
       " (u'bikes', 1),\n",
       " (u'signs', 1),\n",
       " (u'bunch', 1),\n",
       " (u'talking', 1),\n",
       " (u'snowboarder', 1),\n",
       " (u'cup', 1),\n",
       " (u'young', 1),\n",
       " (u'going', 1),\n",
       " (u'elephant', 1),\n",
       " (u'dark', 1),\n",
       " (u'cutting', 1),\n",
       " (u'sun.', 1),\n",
       " (u'tiled', 1),\n",
       " (u'using', 1),\n",
       " (u'little', 1),\n",
       " (u'school', 1),\n",
       " (u'gras', 1),\n",
       " (u'skis', 1),\n",
       " (u'mirror', 1),\n",
       " (u'night', 1),\n",
       " (u'table.', 1),\n",
       " (u'side', 1),\n",
       " (u'glasses', 1),\n",
       " (u'tarmac', 1),\n",
       " (u'brushing', 1),\n",
       " (u'back', 1),\n",
       " (u'past', 1),\n",
       " (u'video', 1),\n",
       " (u'sandy', 1),\n",
       " (u'boy', 1),\n",
       " (u'microwave', 1),\n",
       " (u'various', 1),\n",
       " (u'dirt', 1),\n",
       " (u'screen', 1),\n",
       " (u'horse', 1),\n",
       " (u'water', 1),\n",
       " (u'yard', 1),\n",
       " (u'computers', 1),\n",
       " (u'pole', 1),\n",
       " (u'place', 1),\n",
       " (u'stand', 1),\n",
       " (u'ski', 1),\n",
       " (u'vegetables', 1),\n",
       " (u'sticking', 1),\n",
       " (u'buses', 1),\n",
       " (u'wall', 1),\n",
       " (u'Two', 1),\n",
       " (u'bath', 1),\n",
       " (u'climbing', 1),\n",
       " (u'open', 1),\n",
       " (u'grazing', 1),\n",
       " (u'jet', 1),\n",
       " (u'legs', 1),\n",
       " (u'banana', 1),\n",
       " (u'store', 1),\n",
       " (u'engine', 1),\n",
       " (u'park', 1),\n",
       " (u'glass', 1),\n",
       " (u'fighter', 1),\n",
       " (u'middle', 1),\n",
       " (u'room.', 1),\n",
       " (u'runway', 1),\n",
       " (u'attached', 1),\n",
       " (u'air', 1),\n",
       " (u'kite', 1),\n",
       " (u'display', 1),\n",
       " (u'grassy', 1),\n",
       " (u'flat', 1),\n",
       " (u'jetliner', 1),\n",
       " (u'mouse', 1),\n",
       " (u'car', 1),\n",
       " (u'mountain', 1),\n",
       " (u'Frisbee', 1),\n",
       " (u'floor', 1),\n",
       " (u'holds', 1),\n",
       " (u'couch', 1),\n",
       " (u'shelf', 1),\n",
       " (u'picture', 1),\n",
       " (u'frisbee', 1),\n",
       " (u'hand', 1),\n",
       " (u'vases', 1),\n",
       " (u'pair', 1),\n",
       " (u'passengers', 1),\n",
       " (u'surfing', 1),\n",
       " (u'remote', 1),\n",
       " (u'overlooking', 1),\n",
       " (u'counter', 1),\n",
       " (u'teeth', 1),\n",
       " (u'tall', 1),\n",
       " (u'left', 1),\n",
       " (u'lying', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.most_common_words(predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_words2app = most_common_words(predicted_captions,500)\n",
    "common_words2app = [(word,app) for word,app in common_words2app if word.lower() not in stopwords.words('english')]\n",
    "common_words2app = [(word,app) for word,app in common_words2app if word not in ['START','END']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u' ', 1408),\n",
       " (u'n', 425),\n",
       " (u'e', 381),\n",
       " (u'r', 239),\n",
       " (u'l', 219),\n",
       " (u'g', 169),\n",
       " (u'h', 168),\n",
       " (u'p', 129),\n",
       " (u'E', 128),\n",
       " (u'N', 128),\n",
       " (u'R', 128),\n",
       " (u'w', 122),\n",
       " (u'f', 106),\n",
       " (u'b', 104),\n",
       " (u'c', 102),\n",
       " (u'u', 79),\n",
       " (u'k', 61),\n",
       " (u'v', 33),\n",
       " (u'x', 13),\n",
       " (u'z', 10),\n",
       " (u'.', 3),\n",
       " (u'q', 3),\n",
       " (u'V', 2),\n",
       " (u'j', 2),\n",
       " (u'F', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words2app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searched_word = \"teeth\"\n",
    "(found_images,found_captions) = search_images_by(searched_word,images,predicted_captions)\n",
    "print(\"Number of results = %d\"%len(found_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(found_images,found_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on misc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images_path = save_path + misc_images_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misc_images = []\n",
    "for img_path in os.listdir(misc_images_path):\n",
    "    img = PIL.Image.open(misc_images_path+img_path)\n",
    "    img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "    img = np.asarray(img)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    \n",
    "    misc_images.append(img)\n",
    "    \n",
    "stacked_images = np.vstack(misc_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(misc_images,misc_predicted_captions) = make_prediction_on_dataset(stacked_images)\n",
    "preproc.plot_predictions(misc_images,misc_predicted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
