{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from utils import *\n",
    "import cPickle as pickle\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageData(object):\n",
    "\n",
    "    def __init__(self,id,name):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.captions = []\n",
    "        self.image = []\n",
    "        \n",
    "    def appendCaption(self,caption):\n",
    "        self.captions.append(caption)\n",
    "        \n",
    "class ImageEntry(object):\n",
    "\n",
    "    def __init__(self,image,caption):\n",
    "        self.image = image\n",
    "        self.caption = caption\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data_dict():\n",
    "    annotation_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/raw_annotations/captions_val2014.json\"\n",
    "    with open(annotation_path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        \n",
    "    id2ImageDataDict = {imageJson[\"id\"]: ImageData(imageJson[\"id\"],imageJson[\"file_name\"]) \n",
    "                        for imageJson in data[\"images\"]}\n",
    "    \n",
    "    annotationsJson = data[\"annotations\"]\n",
    "    \n",
    "    for annotationJson in annotationsJson:\n",
    "        imageData = id2ImageDataDict[annotationJson[\"image_id\"]]\n",
    "        caption = annotationJson[\"caption\"]\n",
    "        imageData.appendCaption(caption)\n",
    "\n",
    "    fileName2ImageDataDict = {imageJson[\"file_name\"]: id2ImageDataDict[imageJson[\"id\"]] for imageJson in data[\"images\"]}\n",
    "\n",
    "    return fileName2ImageDataDict\n",
    "\n",
    "def construct_image_data_arr(base_path,fileName2ImageDataDict):   \n",
    "    \n",
    "    image_paths = [f for f in listdir(base_path)]\n",
    "    \n",
    "    \n",
    "    for image_file_name in tqdm(image_paths):\n",
    "        \n",
    "        img = PIL.Image.open(base_path+\"/\"+image_file_name)\n",
    "        img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "        \n",
    "        image_data = fileName2ImageDataDict[image_file_name]\n",
    "        \n",
    "        img = np.asarray(img)\n",
    "        \n",
    "        image_data.image = img\n",
    "        image_data.image = np.asarray(image_data.image)\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_image_data = [imageData for _,imageData in fileName2ImageDataDict.iteritems()]\n",
    "    \n",
    "    filtered_image_data = [imageData for imageData in all_image_data\n",
    "                      if np.asarray(imageData.image).shape == (224,224,3)]\n",
    "    \n",
    "\n",
    "    return  filtered_image_data\n",
    "\n",
    "def constructImageEntryArr(imageDataArr):\n",
    "    image_entry_arr = []\n",
    "\n",
    "    for imageData in imageDataArr:\n",
    "        image = imageData.image\n",
    "\n",
    "        for caption in imageData.captions:\n",
    "            image_entry_arr.append(ImageEntry(image,caption))\n",
    "\n",
    "    return image_entry_arr\n",
    "\n",
    "def construct_images_concat_t(image_data_arr):\n",
    "    image_np_arr = [ np.expand_dims(image_data.image, axis=0) for image_data in image_data_arr]\n",
    "    images_concat =  np.vstack(image_np_arr)\n",
    "    images_concat_t = np.transpose(images_concat,(0,3,1,2))\n",
    "    return images_concat_t\n",
    "\n",
    "\n",
    "def get_unique_words(captions):\n",
    "    unique_words = []\n",
    "    words = [caption.split() for caption in captions]\n",
    "   \n",
    "    for word in words:\n",
    "        unique_words.extend(word)\n",
    "        \n",
    "    unique_words = list(set(unique_words))\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "def get_index_word_dicts(unique_words):\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for i,word in enumerate(unique_words):\n",
    "        word_index[word] = i\n",
    "        index_word[i] = word\n",
    "        \n",
    "    return (word_index,index_word)\n",
    "\n",
    "def get_train_captions_indexed(captions, word2index, MAX_CAPTION_LEN ):\n",
    "    \n",
    "    train_captions_indexed = []\n",
    "    for caption in captions:\n",
    "        one = [word2index[caption_word] for caption_word in caption.split()]\n",
    "        train_captions_indexed.append(one)\n",
    "\n",
    "    train_captions_indexed = sequence.pad_sequences(train_captions_indexed, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "    return train_captions_indexed\n",
    "    \n",
    "def get_captions_from_batch(path,batch_nr):\n",
    "    captions = pickle.load(open(path+\"captions_batch_\"+str(batch_nr)+\".p\", \"rb\" ))\n",
    "    return captions\n",
    "\n",
    "def get_truncated_captions_from_batch(path,batch_nr,nr_instances):\n",
    "    captions = get_captions_from_batch(path,batch_nr)\n",
    "    captions = captions[:nr_instances]\n",
    "    return captions\n",
    "\n",
    "def write_captions_to_disk(path,image_data_arr):\n",
    "    for i in tqdm(range(5)):\n",
    "        captions = [\"START \"+image_data.captions[i]+\" END\" for image_data in image_data_arr] \n",
    "        pickle.dump( captions, open(path+\"captions_batch_\"+str(i)+\".p\", \"wb\" ) )\n",
    "        \n",
    "        \n",
    "def compute_partial_all_words_2_next_word(captions_indexed,step_size,np_save_path):\n",
    "    \n",
    "    for window_start in tqdm(range(0,len(captions_indexed),step_size)):\n",
    "    \n",
    "        captions_indexed_batch = captions_indexed[window_start:window_start+step_size]\n",
    "\n",
    "        all_words_2_next_word = []\n",
    "\n",
    "        for caption_indexed in captions_indexed_batch:\n",
    "\n",
    "            word_2_next_word = []\n",
    "\n",
    "            enhanced_caption_indexed = np.append(caption_indexed,[word2index[\"END\"]]) #hacky\n",
    "\n",
    "            for i in xrange(0,len(caption_indexed)):\n",
    "                caption_word_index = enhanced_caption_indexed[i]\n",
    "                future_word_index = enhanced_caption_indexed[i+1]\n",
    "                future_indexes = np.zeros(VOCAB_SIZE)\n",
    "                future_indexes[future_word_index] = 1\n",
    "\n",
    "                word_2_next_word.append(future_indexes)\n",
    "\n",
    "            words_2_next_word = np.vstack(word_2_next_word)\n",
    "\n",
    "            all_words_2_next_word.append(words_2_next_word)\n",
    "\n",
    "        save_array(np_save_path+ 'all_words_2_next_word__'+str(format(window_start, \"06\"))+'.bc', all_words_2_next_word)\n",
    "    \n",
    "def get_future_words(np_save_path):\n",
    "    all_words_2_next_word_paths= [f for f in listdir(np_save_path)]\n",
    "    all_words_2_next_word_paths.sort()\n",
    "\n",
    "    all_words_2_next_word = [load_array(np_save_path + all_words_2_next_word_path) \n",
    "                             for all_words_2_next_word_path in all_words_2_next_word_paths ]\n",
    "\n",
    "    future_words = np.vstack(all_words_2_next_word)\n",
    "    future_words = np.transpose(future_words,(0,1,2))\n",
    "    return future_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/\"\n",
    "images_path = save_path+\"raw_images/val2014\"\n",
    "image_data_arr_path = save_path+\"imageDataArr/\"\n",
    "images_concat_t_path = save_path+\"imagesConcatT/\"\n",
    "captions_path = save_path+\"captions/\"\n",
    "np_save_path = save_path+\"temp/\"\n",
    "model_path = save_path+\"models/\"\n",
    "images_vgg_features_path = save_path + \"images_vgg_features/\"\n",
    "\n",
    "train_path = save_path + \"train/\"\n",
    "test_path = save_path + \"test/\"\n",
    "\n",
    "\n",
    "images_concat_folder = \"images_concat/\"\n",
    "images_vgg_4096_folder = \"images_vgg_4096/\"\n",
    "captions_folder = \"captions/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_data_arr(images_path):\n",
    "    fileName_2_image_data_dict = build_data_dict()\n",
    "    image_data_arr = construct_image_data_arr(images_path,fileName_2_image_data_dict)\n",
    "    return image_data_arr\n",
    "\n",
    "\n",
    "def get_train_test_data(image_data_arr, test_size):\n",
    "    train_image_data_arr = image_data_arr[test_size:]\n",
    "    test_image_data_arr = image_data_arr[:test_size]\n",
    "    return (train_image_data_arr,test_image_data_arr)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 39438 Test 1000\n"
     ]
    }
   ],
   "source": [
    "# image_data_arr = get_image_data_arr(images_path)\n",
    "(train_image_data_arr,test_image_data_arr) = get_train_test_data(image_data_arr,TEST_SIZE)\n",
    "print(\"Train: %d Test %d\"%(len(train_image_data_arr),len(test_image_data_arr)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images_concat_t = construct_images_concat_t(train_image_data_arr)\n",
    "test_images_concat_t = construct_images_concat_t(test_image_data_arr)\n",
    "\n",
    "print(train_images_concat_t.shape)\n",
    "print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_array(train_path + images_concat_folder+ 'images_concat.bc', train_images_concat_t)\n",
    "# save_array(test_path + images_concat_folder+ 'images_concat.bc', test_images_concat_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data - Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 400.54it/s]\n"
     ]
    }
   ],
   "source": [
    "write_captions_to_disk(train_path + captions_folder, train_image_data_arr)\n",
    "write_captions_to_disk(test_path + captions_folder, test_image_data_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images_concat_t = load_array(train_path + images_concat_folder+ 'images_concat.bc')\n",
    "test_images_concat_t = load_array(test_path + images_concat_folder+ 'images_concat.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39438, 3, 224, 224)\n",
      "(1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_concat_t.shape)\n",
    "print(test_images_concat_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NR_TRAIN_INSTANCES = len(train_images_concat_t)\n",
    "NR_TRAIN_INSTANCES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "train_images_concat_t = train_images_concat_t[:NR_INSTANCES]\n",
    "print(train_images_concat_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Serialized Data - Image VGG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_precomputed_vgg_features = load_array(images_vgg_features_path+'images_vgg_features.bc')\n",
    "images_precomputed_vgg_features = images_precomputed_vgg_features[:NR_INSTANCES]\n",
    "test_images_precomputed_vgg_features = load_array(images_vgg_features_path+'images_vgg_features_last_1000.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(images_precomputed_vgg_features.shape)\n",
    "print(test_images_precomputed_vgg_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train captions: 5000 \n",
      "Test Captions 1000\n"
     ]
    }
   ],
   "source": [
    "train_captions = get_truncated_captions_from_batch(train_path + captions_folder, batch_nr = 0, nr_instances = NR_INSTANCES )\n",
    "test_captions = get_truncated_captions_from_batch(test_path + captions_folder, batch_nr = 0, nr_instances = NR_INSTANCES )\n",
    "print\"Train captions: %d \\nTest Captions %d\"%(len(train_captions),len(test_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train caption shape (5000, 35) \n",
      "Test caption shape (1000, 35) \n"
     ]
    }
   ],
   "source": [
    "all_captions = train_captions+test_captions\n",
    "                \n",
    "MAX_CAPTION_LEN = max([len(caption.split()) for caption in all_captions])\n",
    "\n",
    "unique_words = get_unique_words(all_captions)\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "(word2index, index2word) = get_index_word_dicts(unique_words)\n",
    "\n",
    "train_captions_indexed = get_train_captions_indexed(train_captions, word2index, MAX_CAPTION_LEN )\n",
    "test_captions_indexed = get_train_captions_indexed(test_captions, word2index, MAX_CAPTION_LEN )\n",
    "\n",
    "print(\"Train caption shape %s \\nTest caption shape %s \"%(str(train_captions_indexed.shape),str(test_captions_indexed.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR_TRAIN_INSTANCES = 5000\n",
      "MAX_CAPTION_LEN = 35\n",
      "VOCAB_SIZE = 5085\n"
     ]
    }
   ],
   "source": [
    "print(\"NR_TRAIN_INSTANCES = %s\" % NR_TRAIN_INSTANCES)\n",
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_partial_all_words_2_next_word(train_captions_indexed,STEP_SIZE,np_save_path)\n",
    "train_future_words = get_future_words(np_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "future_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (40438, 259, 13601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vgg_model():\n",
    "    image_model = Vgg16().model\n",
    "    image_model.pop()\n",
    "    image_model.pop()\n",
    "    image_model.trainable = False\n",
    "    image_model.add(RepeatVector(MAX_CAPTION_LEN))\n",
    "    return image_model\n",
    "\n",
    "def get_precomputed_input_model():\n",
    "    input_model = Sequential()\n",
    "    input_model.add(RepeatVector(MAX_CAPTION_LEN,input_shape=(4096,)))\n",
    "    return input_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_language_model():\n",
    "    language_model = Sequential()\n",
    "    language_model.add(Embedding(VOCAB_SIZE, 256, input_length=MAX_CAPTION_LEN))\n",
    "    return language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(image_model,language_model):\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, language_model], mode='concat'))\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE, activation = 'softmax')))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_model = get_vgg_model()\n",
    "# image_model = get_precomputed_input_model()\n",
    "language_model = get_language_model()\n",
    "model = build_model(image_model,language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "language_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"Images : \"+str(train_images_concat_t.shape))\n",
    "print(\"Precomputed features : \"+str(images_precomputed_vgg_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Partial captions : \" + str(train_captions_indexed.shape))\n",
    "print(\"Future words :\" + str(future_words.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_image_model_input = train_images_concat_t\n",
    "train_image_model_input = images_precomputed_vgg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit([train_image_model_input, train_captions_indexed], future_words, batch_size=64, nb_epoch=1, \n",
    "          callbacks = [keras.callbacks.TensorBoard(log_dir=\"/home/docker/fastai-courses/deeplearning1/nbs/tf-logs\", \n",
    "                                                  histogram_freq=0, write_graph=True, write_images=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit([train_image_model_input, train_captions_indexed], future_words, batch_size=64, nb_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate([train_image_model_input, train_captions_indexed], future_words, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'val_1000_precomp_features_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.load_weights(model_path+'val_1000_precomp_features.h5')\n",
    "model.load_weights(model_path+'val_1000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(np.average(image_model.get_weights()[0].shape))\n",
    "print(np.average(language_model.get_weights()[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(np.average(image_model.get_weights()[0].shape))\n",
    "print(np.average(language_model.get_weights()[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_model(model):\n",
    "    vgg_model = get_vgg_model()\n",
    "    test_model = Sequential()\n",
    "    test_model.add(Merge([vgg_model, language_model], mode='concat'))\n",
    "    for layer in model.layers[1:]:\n",
    "        test_model.add(layer)\n",
    "        \n",
    "    test_model.compile(loss='categorical_crossentropy', optimizer = Adam(0.001))\n",
    "    return test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model = get_test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NO_TEST_IMAGES = 10\n",
    "TEST_WINDOW_START = 20\n",
    "\n",
    "test_model = get_test_model(language_model)\n",
    "# test_model = model\n",
    "\n",
    "images2Captions = [make_prediction(i,test_model) \n",
    "                   for i in range(TEST_WINDOW_START,TEST_WINDOW_START+NO_TEST_IMAGES)]\n",
    "images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "captions = [image2Caption[1] for image2Caption in images2Captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(images,titles = captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
