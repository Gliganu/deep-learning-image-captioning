{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers import Embedding,GRU,TimeDistributed,RepeatVector,Merge\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from utils import *\n",
    "import cPickle as pickle\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageData(object):\n",
    "\n",
    "    def __init__(self,id,name):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.captions = []\n",
    "        self.image = []\n",
    "        \n",
    "    def appendCaption(self,caption):\n",
    "        self.captions.append(caption)\n",
    "        \n",
    "class ImageEntry(object):\n",
    "\n",
    "    def __init__(self,image,caption):\n",
    "        self.image = image\n",
    "        self.caption = caption\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data_dict():\n",
    "    annotation_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/raw_annotations/captions_val2014.json\"\n",
    "    with open(annotation_path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        \n",
    "    id2ImageDataDict = {imageJson[\"id\"]: ImageData(imageJson[\"id\"],imageJson[\"file_name\"]) \n",
    "                        for imageJson in data[\"images\"]}\n",
    "    \n",
    "    annotationsJson = data[\"annotations\"]\n",
    "    \n",
    "    for annotationJson in annotationsJson:\n",
    "        imageData = id2ImageDataDict[annotationJson[\"image_id\"]]\n",
    "        caption = annotationJson[\"caption\"]\n",
    "        imageData.appendCaption(caption)\n",
    "\n",
    "    fileName2ImageDataDict = {imageJson[\"file_name\"]: id2ImageDataDict[imageJson[\"id\"]] for imageJson in data[\"images\"]}\n",
    "\n",
    "    return fileName2ImageDataDict\n",
    "\n",
    "def construct_image_data_arr(base_path,fileName2ImageDataDict):   \n",
    "    \n",
    "    image_paths = [f for f in listdir(base_path)]\n",
    "    \n",
    "    \n",
    "    for image_file_name in tqdm(image_paths):\n",
    "        \n",
    "        img = PIL.Image.open(base_path+\"/\"+image_file_name)\n",
    "        img = img.resize((224, 224), PIL.Image.NEAREST)\n",
    "        \n",
    "        image_data = fileName2ImageDataDict[image_file_name]\n",
    "        \n",
    "        img = np.asarray(img)\n",
    "        \n",
    "        image_data.image = img\n",
    "        image_data.image = np.asarray(image_data.image)\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_image_data = [imageData for _,imageData in fileName2ImageDataDict.iteritems()]\n",
    "    \n",
    "    filtered_image_data = [imageData for imageData in all_image_data\n",
    "                      if np.asarray(imageData.image).shape == (224,224,3)]\n",
    "    \n",
    "\n",
    "    return  filtered_image_data\n",
    "\n",
    "def constructImageEntryArr(imageDataArr):\n",
    "    image_entry_arr = []\n",
    "\n",
    "    for imageData in imageDataArr:\n",
    "        image = imageData.image\n",
    "\n",
    "        for caption in imageData.captions:\n",
    "            image_entry_arr.append(ImageEntry(image,caption))\n",
    "\n",
    "    return image_entry_arr\n",
    "\n",
    "def construct_images_concat_t(image_data_arr):\n",
    "    image_np_arr = [ np.expand_dims(image_data.image, axis=0) for image_data in image_data_arr]\n",
    "    images_concat =  np.vstack(image_np_arr)\n",
    "    images_concat_t = np.transpose(images_concat,(0,3,1,2))\n",
    "    return images_concat_t\n",
    "\n",
    "\n",
    "def get_unique_words(captions):\n",
    "    unique_words = []\n",
    "    words = [caption.split() for caption in captions]\n",
    "   \n",
    "    for word in words:\n",
    "        unique_words.extend(word)\n",
    "        \n",
    "    unique_words = list(set(unique_words))\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "def get_index_word_dicts(unique_words):\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for i,word in enumerate(unique_words):\n",
    "        word_index[word] = i\n",
    "        index_word[i] = word\n",
    "        \n",
    "    return (word_index,index_word)\n",
    "\n",
    "def get_train_captions_indexed(captions, word2index, MAX_CAPTION_LEN ):\n",
    "    \n",
    "    train_captions_indexed = []\n",
    "    for caption in captions:\n",
    "        one = [word2index[caption_word] for caption_word in caption.split()]\n",
    "        train_captions_indexed.append(one)\n",
    "\n",
    "    train_captions_indexed = sequence.pad_sequences(train_captions_indexed, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "    return train_captions_indexed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = \"/home/docker/fastai-courses/deeplearning1/nbs/persistent/coco/\"\n",
    "images_path = save_path+\"raw_images/val2014\"\n",
    "image_data_arr_path = save_path+\"imageDataArr/\"\n",
    "images_concat_t_path = save_path+\"imagesConcatT/\"\n",
    "captions_path = save_path+\"captions/\"\n",
    "np_save_path = save_path+\"temp/\"\n",
    "model_path = save_path+\"models/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName_2_image_data_dict = build_data_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_data_arr = construct_image_data_arr(images_path,fileName_2_image_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_images_concat_t = construct_images_concat_t(image_data_arr)\n",
    "train_images_concat_t = load_array(images_concat_t_path + 'val_imagesConcatT_1000.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images_concat_t = load_array(images_concat_t_path+'val_imagesConcatT_last_1000.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_array(images_concat_t_path+ 'val_imagesConcatT.bc', imagestConcatT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NR_INSTANCES = len(imagestConcatT)\n",
    "NR_INSTANCES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "train_images_concat_t = train_images_concat_t[:NR_INSTANCES]\n",
    "print(train_images_concat_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_truncated_captions_from_batch(batch_nr,nr_instances):\n",
    "    captions = pickle.load(open(captions_path+\"val2014_captions_\"+str(batch_nr)+\".p\", \"rb\" ))\n",
    "    captions = captions[:NR_INSTANCES]\n",
    "    return captions\n",
    "\n",
    "def dump_captions_to_disk(image_data_arr):\n",
    "    for i in tqdm(range(5)):\n",
    "        captions = [\"START \"+image_data.captions[i]+\" END\" for image_data in image_data_arr] \n",
    "        pickle.dump( captions, open(captions_path+\"val2014_captions_\"+str(i)+\".p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = get_truncated_captions_from_batch(batch_nr = 0, nr_instances = NR_INSTANCES )\n",
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CAPTION_LEN = max([len(caption.split()) for caption in captions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = get_unique_words(captions)\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "(word2index, index2word) = get_index_word_dicts(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions_indexed = get_train_captions_indexed(captions, word2index, MAX_CAPTION_LEN )\n",
    "train_captions_indexed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR_INSTANCES = 1000\n",
      "MAX_CAPTION_LEN = 30\n",
      "VOCAB_SIZE = 1826\n"
     ]
    }
   ],
   "source": [
    "print(\"NR_INSTANCES = %s\" % NR_INSTANCES)\n",
    "print(\"MAX_CAPTION_LEN = %s\"%MAX_CAPTION_LEN)\n",
    "print(\"VOCAB_SIZE = %s\"%VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_partial_all_words_2_next_word(captions_indexed,step_size,np_save_path):\n",
    "    \n",
    "    for window_start in tqdm(range(0,len(captions_indexed),step_size)):\n",
    "    \n",
    "        captions_indexed_batch = captions_indexed[window_start:window_start+step_size]\n",
    "\n",
    "        all_words_2_next_word = []\n",
    "\n",
    "        for caption_indexed in captions_indexed_batch:\n",
    "\n",
    "            word_2_next_word = []\n",
    "\n",
    "            enhanced_caption_indexed = np.append(caption_indexed,[word2index[\"END\"]]) #hacky\n",
    "\n",
    "            for i in xrange(0,len(caption_indexed)):\n",
    "                caption_word_index = enhanced_caption_indexed[i]\n",
    "                future_word_index = enhanced_caption_indexed[i+1]\n",
    "                future_indexes = np.zeros(VOCAB_SIZE)\n",
    "                future_indexes[future_word_index] = 1\n",
    "\n",
    "                word_2_next_word.append(future_indexes)\n",
    "\n",
    "            words_2_next_word = np.vstack(word_2_next_word)\n",
    "\n",
    "            all_words_2_next_word.append(words_2_next_word)\n",
    "\n",
    "        save_array(np_save_path+ 'all_words_2_next_word__'+str(format(window_start, \"06\"))+'.bc', all_words_2_next_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_future_words(np_save_path):\n",
    "    all_words_2_next_word_paths= [f for f in listdir(np_save_path)]\n",
    "    all_words_2_next_word_paths.sort()\n",
    "\n",
    "    all_words_2_next_word = [load_array(np_save_path + all_words_2_next_word_path) \n",
    "                             for all_words_2_next_word_path in all_words_2_next_word_paths ]\n",
    "\n",
    "    future_words = np.vstack(all_words_2_next_word)\n",
    "    future_words = np.transpose(future_words,(0,1,2))\n",
    "    return future_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00,  9.84it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_partial_all_words_2_next_word(train_captions_indexed,STEP_SIZE,np_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30, 1826)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_words = get_future_words(np_save_path)\n",
    "future_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (40438, 259, 13601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove the last two layers to get the 4096D activations    \n",
    "image_model = Vgg16().model\n",
    "image_model.pop()\n",
    "image_model.pop()\n",
    "image_model.trainable = False\n",
    "image_model.add(RepeatVector(MAX_CAPTION_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "language_model = Sequential()\n",
    "language_model.add(Embedding(VOCAB_SIZE, 256, input_length=MAX_CAPTION_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Merge([image_model, language_model], mode='concat'))\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE, activation = 'softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = Adam(0.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images : (1000, 3, 224, 224)\n",
      "Partial captions : (1000, 30)\n",
      "Future words :(1000, 30, 1826)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images : \"+str(train_images_concat_t.shape))\n",
    "print(\"Partial captions : \" + str(train_captions_indexed.shape))\n",
    "print(\"Future words :\" + str(future_words.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit([train_images_concat_t, train_captions_indexed], future_words, batch_size=64, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate([train_images_concat_t, train_captions_indexed], future_words, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights(model_path+'val_1000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'val_1000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(ims, titles):  \n",
    "    for i in range(len(ims)):\n",
    "        plt.title(titles[i])\n",
    "        plt.imshow(ims[i])\n",
    "        plt.figure()\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "def make_prediction(random_number):\n",
    "    startIndex = word_index[\"START\"]\n",
    "    start_captions = [[startIndex]]\n",
    "    start_captions = sequence.pad_sequences(start_captions, maxlen=MAX_CAPTION_LEN,padding='post')\n",
    "\n",
    "    firstImage = np.expand_dims(newImagesConcatT[random_number], axis=0)\n",
    "    firstCaption = np.expand_dims(start_captions[0], axis=0) \n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    endGenerated = False\n",
    "    i = 0\n",
    "    while ((not endGenerated) & (i < MAX_CAPTION_LEN-1)):\n",
    "    # for i in range(17):\n",
    "        predictions = model.predict([firstImage, firstCaption])\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        currentPred = predictions[i]\n",
    "\n",
    "        max_index = np.argmax(currentPred)\n",
    "\n",
    "    #     top3_max_indexes = predictions.argsort()[-4:][::-1]\n",
    "\n",
    "    #     max_index = top3_max_indexes[3]\n",
    "    #     print(predictions.shape)\n",
    "\n",
    "        outputs.append(max_index)\n",
    "        firstCaption[0,i+1] = max_index\n",
    "\n",
    "        i+=1\n",
    "\n",
    "        if(index_word[max_index] == \"END\"):\n",
    "            endGenerated = True\n",
    "\n",
    "    caption = ' '.join([index_word[x] for x in firstCaption[0][:i+1]])\n",
    "    \n",
    "    drawImage = firstImage[0]\n",
    "    drawImageT = np.transpose(drawImage,(1,2,0))\n",
    "    plt.imshow(drawImageT)\n",
    "    \n",
    "    return (drawImageT,caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NO_TEST_IMAGES = 10\n",
    "TEST_WINDOW_START = 20\n",
    "\n",
    "images2Captions = [make_prediction(i) \n",
    "                   for i in range(TEST_WINDOW_START,TEST_WINDOW_START+NO_TEST_IMAGES)]\n",
    "images = [image2Caption[0] for image2Caption in images2Captions]\n",
    "captions = [image2Caption[1] for image2Caption in images2Captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(images,titles = captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
