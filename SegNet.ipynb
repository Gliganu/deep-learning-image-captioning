{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.layers.core import Activation, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_segnet_model():\n",
    "    img_w = 256\n",
    "    img_h = 256\n",
    "    n_labels = 2\n",
    "\n",
    "    kernel = 3\n",
    "\n",
    "    encoding_layers = [\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same', input_shape=(1, img_h, img_w)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "    ]\n",
    "\n",
    "    autoencoder = models.Sequential()\n",
    "    autoencoder.encoding_layers = encoding_layers\n",
    "\n",
    "    for l in autoencoder.encoding_layers:\n",
    "        autoencoder.add(l)\n",
    "\n",
    "    decoding_layers = [\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(n_labels, 1, 1, border_mode='valid'),\n",
    "        BatchNormalization(),\n",
    "    ]\n",
    "    autoencoder.decoding_layers = decoding_layers\n",
    "    for l in autoencoder.decoding_layers:\n",
    "        autoencoder.add(l)\n",
    "\n",
    "    autoencoder.add(Reshape((n_labels, img_h * img_w)))\n",
    "    autoencoder.add(Permute((2, 1)))\n",
    "    autoencoder.add(Activation('softmax'))\n",
    "\n",
    "    return autoencoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/docker/fastai-courses/deeplearning1/nbs/persistent/fast-ai-notebooks/deep-learning-image-captioning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = '/home/docker/fastai-courses/deeplearning1/nbs/persistent/keras-segnet/model_5l_weight_ep50.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file '/home/docker/fastai-courses/deeplearning1/nbs/persistent/keras-segnet/model_5l_weight_ep50.hdf5', mode 'r' at 0x7f160a0c9a50>\n"
     ]
    }
   ],
   "source": [
    "with open(weights_path, 'r') as outfile:\n",
    "    print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_segnet_model()\n",
    "# model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/docker/fastai-courses/deeplearning1/nbs/persistent/keras-segnet/model_random_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/home/docker/fastai-courses/deeplearning1/nbs/persistent/keras-segnet/model_random_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('model_5l.json') as model_file:\n",
    "    autoencoder = models.model_from_json(model_file.read())\n",
    "\n",
    "optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "autoencoder.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "print 'Compiled: OK'\n",
    "\n",
    "# Train model or load weights\n",
    "# train_data, train_label = prep_data('train')\n",
    "# nb_epoch = 50\n",
    "# batch_size = 18\n",
    "# history = autoencoder.fit(train_data, train_label, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "# autoencoder.save_weights('model_5l_weight_ep50.hdf5')\n",
    "\n",
    "\n",
    "# Model visualization\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(autoencoder, to_file='model.png', show_shapes=True)\n",
    "\n",
    "test_data, test_label = prep_data('test')\n",
    "score = autoencoder.evaluate(test_data, test_label, verbose=0)\n",
    "print 'Test score:', score[0]\n",
    "print 'Test accuracy:', score[1]\n",
    "\n",
    "output = autoencoder.predict_proba(test_data, verbose=0)\n",
    "output = output.reshape((output.shape[0], img_h, img_w, n_labels))\n",
    "\n",
    "plot_results(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
